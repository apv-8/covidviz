{
  
    
        "post0": {
            "title": "Relation b/w median income, average household size & cases per capita in NYC",
            "content": "Today we will make the plots that are present in this webpage that tracks the situation in New York City - New York City Coronavirus Map and Case Count . There are two main plots - . Cases per capita | Plot analyzing the relationship b/w average household size, income and cases per capita | They have since corrected the graph - . Old . New . #hide_output import pandas as pd import altair as alt import geopandas as gpd alt.renderers.set_embed_options(actions=False) . Fortunately NYC Department of Health and Mental Hygiene publishes their data in their GitHub Repo. It has all the data from cases to the shapefiles too. . But we will use the data from NYC Open Data which is equally good. We will use the NYC Department of Health and Mental Hygiene GitHub repo only for their COVID data per MODZCTA. . The location of the dataset on NYC Open Data portal is https://data.cityofnewyork.us/Health/Modified-Zip-Code-Tabulation-Areas-MODZCTA-/pri4-ifjk which we will export as geojson. . import requests resp = requests.get(&#39;https://data.cityofnewyork.us/api/geospatial/pri4-ifjk?method=export&amp;format=GeoJSON&#39;) data = resp.json() nyc_zip = gpd.GeoDataFrame.from_features(data[&#39;features&#39;]) nyc_zip.head() . geometry label modzcta pop_est zcta . 0 MULTIPOLYGON (((-73.98774 40.74407, -73.98819 ... | 10001, 10118 | 10001 | 23072 | 10001, 10119, 10199 | . 1 MULTIPOLYGON (((-73.99750 40.71407, -73.99709 ... | 10002 | 10002 | 74993 | 10002 | . 2 MULTIPOLYGON (((-73.98864 40.72293, -73.98876 ... | 10003 | 10003 | 54682 | 10003 | . 3 MULTIPOLYGON (((-74.00827 40.70772, -74.00937 ... | 10004 | 10004 | 3028 | 10004 | . 4 MULTIPOLYGON (((-74.00783 40.70309, -74.00786 ... | 10005 | 10005 | 8831 | 10005, 10271 | . # If you have the data locally #modzcta = &#39;MODZCTA/Modified Zip Code Tabulation Areas.geojson&#39; #nyc_zip_shp = gpd.read_file(modzcta) . This is how it looks when colored based on population - . alt.Chart(nyc_zip).mark_geoshape().encode( color=&#39;pop_est:Q&#39; ) . Now we will get the Median Household Income data from Census Reporter&#39;s wonderful website. In particular the url query for the 5 boroughs in NYC is https://censusreporter.org/data/table/?table=B19013&amp;geo_ids=860%7C05000US36061,860%7C05000US36047,860%7C05000US36081,860%7C05000US36005,860%7C05000US36085 which I came to know thanks to this comment in NYC Health covid data repo. . Download it and export it to work with it further (I chose the Shapefile, but I suppose others would be fine too). . median_income_path = &#39;MODZCTA/median_income_nyc/acs2018_5yr_B19013_86000US11417/acs2018_5yr_B19013_86000US11417.shp&#39; #push it to datasets repo and link here median_income = gpd.read_file(median_income_path) median_income.head() . geoid name B19013001 B19013001e geometry . 0 86000US10001 | 10001 | 88526.0 | 8060.0 | POLYGON ((-74.00828 40.75027, -74.00783 40.751... | . 1 86000US10002 | 10002 | 35859.0 | 2149.0 | POLYGON ((-73.99750 40.71407, -73.99709 40.714... | . 2 86000US10003 | 10003 | 112131.0 | 13190.0 | POLYGON ((-73.99937 40.73132, -73.99911 40.731... | . 3 86000US10004 | 10004 | 157645.0 | 17195.0 | MULTIPOLYGON (((-73.99814 40.70152, -73.99617 ... | . 4 86000US10005 | 10005 | 173333.0 | 33390.0 | POLYGON ((-74.01251 40.70677, -74.01195 40.707... | . The regions in median income data that are not part of NTC&#39;s modzcta can be seen as follows(basically we are finding out if both datasets show the same regions) - . median_income[median_income[&#39;name&#39;].isin(nyc_zip[&#39;modzcta&#39;]) == False].plot() . &lt;AxesSubplot:&gt; . Fun Fact - the areas marked above are also NOT SHOWN in NYT&#39;s Charts . If you want you can plot the Median Income Data too using - . alt.Chart(median_income).mark_geoshape().encode( color=&#39;B19013001:Q&#39; ) . Now we will get the COVID data per MODZCTA . covid_zip_uri = &#39;https://raw.githubusercontent.com/nychealth/coronavirus-data/master/data-by-modzcta.csv&#39; covid_zip = pd.read_csv(covid_zip_uri) covid_zip.head() . MODIFIED_ZCTA NEIGHBORHOOD_NAME BOROUGH_GROUP COVID_CASE_COUNT COVID_CASE_RATE POP_DENOMINATOR COVID_DEATH_COUNT COVID_DEATH_RATE PERCENT_POSITIVE TOTAL_COVID_TESTS . 0 10001 | Chelsea/NoMad/West Chelsea | Manhattan | 440 | 1867.33 | 23563.03 | 26 | 110.34 | 7.18 | 6124 | . 1 10002 | Chinatown/Lower East Side | Manhattan | 1318 | 1717.14 | 76755.41 | 161 | 209.76 | 8.33 | 15831 | . 2 10003 | East Village/Gramercy/Greenwich Village | Manhattan | 534 | 992.54 | 53801.62 | 35 | 65.05 | 3.91 | 13671 | . 3 10004 | Financial District | Manhattan | 40 | 1095.71 | 3650.61 | 1 | 27.39 | 5.33 | 751 | . 4 10005 | Financial District | Manhattan | 94 | 1119.57 | 8396.11 | 2 | 23.82 | 4.91 | 1914 | . Merge the covid data with NYC shapefile data to be able to see the number of cases per modzcta . def modify_merge(nyc_zip, covid_zip): covid_zip = covid_zip.loc[:, [&#39;MODIFIED_ZCTA&#39;, &#39;BOROUGH_GROUP&#39;, &#39;COVID_CASE_COUNT&#39;, &#39;COVID_DEATH_COUNT&#39;, &#39;NEIGHBORHOOD_NAME&#39;]] nyc_zip[&#39;pop_est&#39;] = nyc_zip[&#39;pop_est&#39;].astype(int) nyc_zip[&#39;modzcta&#39;] = nyc_zip[&#39;modzcta&#39;].astype(int) covid_zip.rename(columns = {&#39;MODIFIED_ZCTA&#39;: &#39;modzcta&#39;}, inplace=True) covid_nyc_zip = nyc_zip.merge(covid_zip, how=&#39;left&#39;, on=&#39;modzcta&#39;) return covid_nyc_zip . covid_nyc_zip = modify_merge(nyc_zip, covid_zip) covid_nyc_zip.head() . geometry label modzcta pop_est zcta BOROUGH_GROUP COVID_CASE_COUNT COVID_DEATH_COUNT NEIGHBORHOOD_NAME . 0 MULTIPOLYGON (((-73.98774 40.74407, -73.98819 ... | 10001, 10118 | 10001 | 23072 | 10001, 10119, 10199 | Manhattan | 440.0 | 26.0 | Chelsea/NoMad/West Chelsea | . 1 MULTIPOLYGON (((-73.99750 40.71407, -73.99709 ... | 10002 | 10002 | 74993 | 10002 | Manhattan | 1318.0 | 161.0 | Chinatown/Lower East Side | . 2 MULTIPOLYGON (((-73.98864 40.72293, -73.98876 ... | 10003 | 10003 | 54682 | 10003 | Manhattan | 534.0 | 35.0 | East Village/Gramercy/Greenwich Village | . 3 MULTIPOLYGON (((-74.00827 40.70772, -74.00937 ... | 10004 | 10004 | 3028 | 10004 | Manhattan | 40.0 | 1.0 | Financial District | . 4 MULTIPOLYGON (((-74.00783 40.70309, -74.00786 ... | 10005 | 10005 | 8831 | 10005, 10271 | Manhattan | 94.0 | 2.0 | Financial District | . Customary covid cases visualization per modfied zip code tabulation area - . alt.Chart(covid_nyc_zip).mark_geoshape().encode( color=&#39;COVID_CASE_COUNT&#39; ) . Now we will merge median income data with the covid cases per zip code tabulation area data we just derieved above - . def modify_merge(covid_nyc_zip, median_income): median_income.rename(columns={&#39;name&#39;: &#39;modzcta&#39;, &#39;B19013001&#39;: &#39;median_income&#39;}, inplace=True) median_income[&#39;modzcta&#39;] = median_income[&#39;modzcta&#39;].astype(int) median_income = median_income.drop(columns=[&#39;geometry&#39;, &#39;B19013001e&#39;]) covid_income_zip = covid_nyc_zip.merge(median_income, how=&#39;inner&#39;, on=&#39;modzcta&#39;) covid_income_zip = covid_income_zip.assign( #case_per_people = covid_income_zip[&#39;COVID_CASE_COUNT&#39;]/covid_income_zip[&#39;POP_DENOMINATOR&#39;], #their earlier calculation case_per_people = covid_income_zip[&#39;COVID_CASE_COUNT&#39;]/covid_income_zip[&#39;pop_est&#39;], # current calculation ) return covid_income_zip . covid_income_zip = modify_merge(covid_nyc_zip, median_income) covid_income_zip.head() . geometry label modzcta pop_est zcta BOROUGH_GROUP COVID_CASE_COUNT COVID_DEATH_COUNT NEIGHBORHOOD_NAME geoid median_income case_per_people . 0 MULTIPOLYGON (((-73.98774 40.74407, -73.98819 ... | 10001, 10118 | 10001 | 23072 | 10001, 10119, 10199 | Manhattan | 440.0 | 26.0 | Chelsea/NoMad/West Chelsea | 86000US10001 | 88526.0 | 0.019071 | . 1 MULTIPOLYGON (((-73.99750 40.71407, -73.99709 ... | 10002 | 10002 | 74993 | 10002 | Manhattan | 1318.0 | 161.0 | Chinatown/Lower East Side | 86000US10002 | 35859.0 | 0.017575 | . 2 MULTIPOLYGON (((-73.98864 40.72293, -73.98876 ... | 10003 | 10003 | 54682 | 10003 | Manhattan | 534.0 | 35.0 | East Village/Gramercy/Greenwich Village | 86000US10003 | 112131.0 | 0.009766 | . 3 MULTIPOLYGON (((-74.00827 40.70772, -74.00937 ... | 10004 | 10004 | 3028 | 10004 | Manhattan | 40.0 | 1.0 | Financial District | 86000US10004 | 157645.0 | 0.013210 | . 4 MULTIPOLYGON (((-74.00783 40.70309, -74.00786 ... | 10005 | 10005 | 8831 | 10005, 10271 | Manhattan | 94.0 | 2.0 | Financial District | 86000US10005 | 173333.0 | 0.010644 | . Now let&#39;s plot this data and we will find that it exactly MATCHES NYT&#39;s chart for cases per people v/s median income - . alt.Chart(covid_income_zip).mark_circle().encode( color=alt.Color(&#39;median_income:Q&#39;, sort = &quot;descending&quot;), x=alt.X(&#39;median_income:Q&#39;), y=&#39;case_per_people:Q&#39; ) . Getting the number of people per household data . This data can be calculated from Household Type by Household Size data, again from Census Reporter - the url query would be https://censusreporter.org/data/table/?table=B11016&amp;geo_ids=860|05000US36061,860|05000US36047,860|05000US36081,860|05000US36005,860|05000US36085 . Getting data like these honestly deserves its own blog post. It took me a very long time to find the proper data. Perhaps I will write about it some other time. . household_path = &#39;MODZCTA/household_type_by_household_size/csvofsame/acs2018_5yr_B11016_86000US11417.csv&#39; household = pd.read_csv(household_path) household.head() . geoid name Total Total, Error Family Households Family Households, Error 2-person household B11016003, Error 3-person household B11016004, Error ... 3-person nonfamily household B11016012, Error 4-person nonfamily household B11016013, Error 5-person nonfamily household B11016014, Error 6-person nonfamily household B11016015, Error 7 or more-person nonfamily household B11016016, Error . 0 86000US10001 | 10001 | 12431 | 521 | 3838 | 413 | 2330 | 338 | 819 | 295 | ... | 253 | 97 | 63 | 55 | 0 | 22 | 0 | 22 | 0 | 22 | . 1 86000US10002 | 10002 | 33540 | 614 | 16565 | 738 | 8090 | 620 | 4287 | 432 | ... | 621 | 157 | 84 | 87 | 0 | 28 | 0 | 28 | 0 | 28 | . 2 86000US10003 | 10003 | 26124 | 703 | 7946 | 551 | 5182 | 472 | 1413 | 316 | ... | 586 | 212 | 94 | 70 | 38 | 38 | 0 | 28 | 0 | 28 | . 3 86000US10004 | 10004 | 1659 | 238 | 709 | 183 | 410 | 160 | 196 | 106 | ... | 42 | 31 | 0 | 12 | 0 | 12 | 0 | 12 | 0 | 12 | . 4 86000US10005 | 10005 | 4374 | 337 | 1614 | 316 | 944 | 260 | 358 | 182 | ... | 199 | 96 | 15 | 25 | 0 | 17 | 0 | 17 | 0 | 17 | . 5 rows × 34 columns . def consolidated_household_per_zip(df): household_data = pd.DataFrame() household_data = household_data.assign( one_person = df[&#39;1-person nonfamily household&#39;], two_person = df[&#39;2-person household&#39;] + df[&#39;2-person nonfamily household&#39;], three_person = df[&#39;3-person household&#39;] + df[&#39;3-person nonfamily household&#39;], four_person = df[&#39;4-person household&#39;] + df[&#39;4-person nonfamily household&#39;], five_person = df[&#39;5-person household&#39;] + df[&#39;5-person nonfamily household&#39;], six_person = df[&#39;6-person household&#39;] + df[&#39;6-person nonfamily household&#39;], seven_or_more_person = df[&#39;7 or more-person household&#39;] + df[&#39;7 or more-person nonfamily household&#39;], modzcta = df[&#39;name&#39;], total = df[&#39;Total&#39;] #avg_people_per_household = df.apply(lambda x: (x[1]+2*x[2]+3*x[3]+4*x[4]+5*x[5]+6*x[6]+7*x[7])/(x[&#39;total&#39;]), axis=1) ) return household_data household_data = consolidated_household_per_zip(household) household_data.head() . one_person two_person three_person four_person five_person six_person seven_or_more_person modzcta total . 0 6710 | 3897 | 1072 | 429 | 307 | 16 | 0 | 10001 | 12431 | . 1 14319 | 10041 | 4908 | 2796 | 1162 | 95 | 219 | 10002 | 33540 | . 2 14377 | 8265 | 1999 | 1367 | 109 | 7 | 0 | 10003 | 26124 | . 3 794 | 524 | 238 | 95 | 8 | 0 | 0 | 10004 | 1659 | . 4 1674 | 1816 | 557 | 314 | 13 | 0 | 0 | 10005 | 4374 | . Now we will merge the household data too to arrive at the final data for plotting - . plot_data = covid_income_zip.merge(household_data, how=&#39;inner&#39;, on=&#39;modzcta&#39;) . Calculating Average people per household is simple enough, jsut divide the population with the number of households - . plot_data[&#39;avg_person_per_household&#39;] = plot_data[&#39;pop_est&#39;]/plot_data[&#39;total&#39;] . Finally let&#39;s plot the data side by side - horizontal concatenatioon using | operator - . income = alt.Chart(plot_data).mark_circle().encode( color=alt.Color(&#39;median_income:Q&#39;, sort = &quot;descending&quot;), x=alt.X(&#39;median_income:Q&#39;), y=&#39;case_per_people:Q&#39; ) household_size = alt.Chart(plot_data).mark_circle().encode( #color=alt.Color(&#39;median_income:Q&#39;, sort = &quot;descending&quot;), x=alt.X(&#39;avg_person_per_household:Q&#39;), y=&#39;case_per_people:Q&#39; ) income|household_size . Let&#39;s beautify it a little and add interactivity so that whenever you select an area in one of the graphs, the corresponding points gets highlighted in the second graph and you can drag the selection around to see that actually that the outbreak is worse in poorer areas where more people live together. . brush = alt.selection_interval(encodings=[&#39;x&#39;]) chart = alt.Chart(plot_data).mark_circle(size=100).encode( color=alt.condition(brush, &#39;BOROUGH_GROUP:N&#39;, alt.value(&#39;lightgray&#39;), legend=alt.Legend(title=&quot;Borough&quot;)), y=alt.Y(&#39;case_per_people:Q&#39;) ).add_selection(brush).properties(width=525, height=480) alt.hconcat( chart.encode( x=alt.X(&#39;median_income:Q&#39;, axis=alt.Axis(tickCount=3, format=&quot;$d&quot;,titleOpacity=0.6, title=&#39;Higher Median Income -&gt;&#39;)), y=alt.Y(&#39;case_per_people:Q&#39;, axis=alt.Axis(format=&quot;%&quot;,titleOpacity=0.6, tickCount=4, title=&quot;Cases in % of population&quot;)) ), chart.encode( x=alt.X(&#39;avg_person_per_household:Q&#39;, scale=alt.Scale(zero=False), axis=alt.Axis(tickCount=3,titleOpacity=0.6, title=&#39;More People per Household -&gt;&#39;)), y=alt.Y(&#39;case_per_people:Q&#39;, axis=alt.Axis(format=&quot;%&quot;, tickCount=4, title=None)) ) ).configure_view(strokeWidth=0).configure_axis(grid=True,gridOpacity=0.6,labelOpacity=0.5) . Way to reproduce the old incorrect graph - . Get the total population data as 1*(1 person households) + .... + 7*(7 or more person households) . Hope you noticed the error over there, but it perfectly captures the old graph as you can see in my old tweet which I had implemented that way - Working on my next set of #dataviz. The regions in NYC with high cases per capita are also regions with the lowest median incomes and largest average household size. Inspired by @nytgraphics and made using #Python and #altair Follow the blog 👉 https://t.co/JfXtHy5hPh pic.twitter.com/QawWEEgb4B . &mdash; Shantam Raj (@RajShantam) July 4, 2020 Gif of the above implementation - . Now let&#39;s make the geospatial chloropleth plot cases per capita like the second chart at the top of the post . Since we have the data in order, - we will use the covid_income_zip data beause it already has the &quot;case_per_people&quot; field that we have to plot - potting is as simple as - . alt.Chart(covid_income_zip).mark_geoshape().encode( color=&#39;case_per_people&#39; ) . Using their color scheme - . def color_it(x): if x&lt;(1/50): return &quot;less than 1/50&quot; elif x&lt;(1/40): return &quot;1/50 to 1/40&quot; elif x&lt;(1/30): return &quot;1/40 to 1/30&quot; else: return &quot;more than 1/30&quot; covid_income_zip = covid_income_zip.assign(share_of_population=covid_income_zip[&#39;case_per_people&#39;].apply(color_it)) alt.Chart(covid_income_zip).mark_geoshape().encode( color=alt.Color(&#39;share_of_population&#39;, scale=alt.Scale(domain=[&#39;less than 1/50&#39;, &#39;1/50 to 1/40&#39;, &#39;1/40 to 1/30&#39;, &#39;more than 1/30&#39;], range=[&#39;#f2df91&#39;, &#39;#ffae43&#39;, &#39;#ff6e0b&#39;, &#39;#ce0a05&#39;])) ).properties(width=600, height=600) . How do we add borough names on top? . I love this part - time for some actual spatial analysis. To show the Borough names we will actually dissolve the areas by borough and get the centroid and finally place our text mark there . borough = covid_income_zip.dissolve(by=&#39;BOROUGH_GROUP&#39;) borough[&#39;lon&#39;] = borough[&#39;geometry&#39;].centroid.x borough[&#39;lat&#39;] = borough[&#39;geometry&#39;].centroid.y borough = borough.reset_index() borough . BOROUGH_GROUP geometry label modzcta pop_est zcta COVID_CASE_COUNT COVID_DEATH_COUNT NEIGHBORHOOD_NAME geoid median_income case_per_people share_of_population lon lat . 0 Bronx | MULTIPOLYGON (((-73.84290 40.82857, -73.84287 ... | 10451 | 10451 | 47798 | 10451 | 1737.0 | 152.0 | Concourse/Melrose | 86000US10451 | 28921.0 | 0.036340 | more than 1/30 | -73.866550 | 40.853688 | . 1 Brooklyn | POLYGON ((-73.93190 40.59469, -73.93193 40.594... | 11201 | 11201 | 62823 | 11201 | 818.0 | 95.0 | Brooklyn Heights/DUMBO/Downtown Brooklyn | 86000US11201 | 124996.0 | 0.013021 | less than 1/50 | -73.949561 | 40.645380 | . 2 Manhattan | MULTIPOLYGON (((-74.00760 40.70299, -74.00765 ... | 10001, 10118 | 10001 | 23072 | 10001, 10119, 10199 | 440.0 | 26.0 | Chelsea/NoMad/West Chelsea | 86000US10001 | 88526.0 | 0.019071 | less than 1/50 | -73.966228 | 40.778413 | . 3 Queens | MULTIPOLYGON (((-73.86496 40.56663, -73.86509 ... | 11004, 11005 | 11004 | 19216 | 11004, 11005, 11040 | 633.0 | 68.0 | Bellerose/Douglaston-Little Neck | 86000US11004 | 92657.0 | 0.032941 | 1/40 to 1/30 | -73.819049 | 40.710385 | . 4 Staten Island | POLYGON ((-74.20999 40.51055, -74.21013 40.510... | 10301 | 10301 | 38733 | 10301 | 1314.0 | 107.0 | Silver Lake/St. George | 86000US10301 | 55802.0 | 0.033925 | more than 1/30 | -74.153793 | 40.581313 | . Let&#39;s plot it and put the legend on top - . a = alt.Chart(covid_income_zip).mark_geoshape().encode( color=alt.Color( &#39;share_of_population&#39;, scale=alt.Scale(domain=[&#39;less than 1/50&#39;, &#39;1/50 to 1/40&#39;, &#39;1/40 to 1/30&#39;, &#39;more than 1/30&#39;], range=[&#39;#f2df91&#39;, &#39;#ffae43&#39;, &#39;#ff6e0b&#39;, &#39;#ce0a05&#39;]), legend=alt.Legend(title=&quot;Share of Population&quot;, orient=&#39;top&#39;, symbolType=&#39;square&#39;, columnPadding=20, symbolSize=200) ), tooltip = [&#39;BOROUGH_GROUP&#39;, &#39;NEIGHBORHOOD_NAME&#39;, &#39;modzcta&#39;, &#39;COVID_CASE_COUNT&#39;, &#39;COVID_DEATH_COUNT&#39;] ).properties(width=600, height=600) b = alt.Chart(borough).mark_text().encode( longitude=&#39;lon&#39;, latitude=&#39;lat&#39;, text=&#39;BOROUGH_GROUP:N&#39; ) a+b .",
            "url": "https://armsp.github.io/covidviz/interactive/geospatial/nyt/2020/08/23/NYC-covid-cases-income-people-per-household.html",
            "relUrl": "/interactive/geospatial/nyt/2020/08/23/NYC-covid-cases-income-people-per-household.html",
            "date": " • Aug 23, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Decrease in GDP of USA",
            "content": "Tis post will show you how to make the GDP chart in the article by titled Big Tech Earnings Surge as Economy Slumps . . Nothing that it&#39;s a percent change from previous quarter data, we will extract the data for this chart as follows from Bureau of Economic Analysis - . Go here - https://apps.bea.gov/iTable/index_nipa.cfm | Click on &quot;Begin using this data&quot; | Click on Section 1 | Click on Table 1.1.1 - Percent Change From Preceding Period in Real Gross Domestic Product - Annualized | Click on Modify | Select on all years | Repeat the steps for Table 1.1.3 - Real Gross Domestic Product, Quantity Indexes | . There are however some important things to know before going further which will save you a lot of time and give you a better understanding of what you are doing, which I did not understand the first time and had to dig up a lot to figure. Read this article and then this. . Now you know that GDP is reported in a rather peculiar way - annulazied GDP. It mean&#39;s they report values of GDP such that if the current state of affairs continue then what would happen by the end of the year . The formula for annunalizing is - . $ g_{m} = left[ left( frac{X_{m}}{X_{m-1}} right)^n -1 right] cdot100 $ . where n is 4 for quarterly available data and 12 for monthly data . import pandas as pd import altair as alt from functools import wraps import datetime as dt alt.renderers.set_embed_options(actions=False) def log_step(func): @wraps(func) def wrapper(*args, **kwargs): &quot;&quot;&quot;timing or logging etc&quot;&quot;&quot; start = dt.datetime.now() output = func(*args, **kwargs) end = dt.datetime.now() print(f&quot;After function {func.__name__} ran, shape of dataframe is - {output.shape}, execution time is - {end-start}&quot;) return output return wrapper @log_step def read_transpose_gdp_data(path, if_uri): if if_uri: pass else: cols = pd.read_csv(path, skiprows=4, header=None, nrows=1) us_gdp = pd.read_csv(path, skiprows=4, header=None, usecols=[i for i in cols if i != 0]) us_gdp = us_gdp.T us_gdp.iloc[0,0] = &#39;year&#39; us_gdp.iloc[0,1] = &#39;quarter&#39; us_gdp.columns = us_gdp.iloc[0] us_gdp = us_gdp[1:] return us_gdp @log_step def clean_gdp_data(df): df.columns = [x.strip() for x in df.columns] #df.columns = [x.strip() if type(x) != float else x for x in list(df.columns)] df[&#39;Gross domestic product&#39;] = df[&#39;Gross domestic product&#39;].astype(float) df[&#39;year&#39;] = df[&#39;year&#39;].astype(int) df = df.reset_index(drop=True) #df.rename(columns={&#39;Gross domestic product&#39;: &#39;gdp&#39;}, inplace=True) #df[&#39;year&#39;] = pd.to_datetime(df[&#39;year&#39;], format=&quot;%Y&quot;) df.drop(df.columns[3:], inplace=True, axis=1) return df @log_step def rename_cols(df, col_dict): df.rename(columns=col_dict, inplace=True) return df @log_step def assign_columns(df): df = df.assign(non_annualized_gdp = df[&#39;real_gdp&#39;].diff()/df[&#39;real_gdp&#39;].shift(1)*100) return df @log_step def reshape_concat_column(df, col): df = df[1:].reset_index() df = df.assign(annualized_gdp = col) return df def year_as_time(df): df[&#39;year&#39;] = pd.to_datetime(df[&#39;year&#39;], format=&quot;%Y&quot;) return df . Annualized GDP . ann_gdp = (read_transpose_gdp_data(path=&#39;usa_gdp.csv&#39;, if_uri=False) .pipe(clean_gdp_data) .pipe(rename_cols, col_dict={&#39;Gross domestic product&#39;: &#39;ann_gdp&#39;}) ) ann_gdp.head() . After function read_transpose_gdp_data ran, shape of dataframe is - (293, 30), execution time is - 0:00:00.080267 After function clean_gdp_data ran, shape of dataframe is - (293, 3), execution time is - 0:00:00.004031 After function rename_cols ran, shape of dataframe is - (293, 3), execution time is - 0:00:00.000345 . year quarter ann_gdp . 0 1947 | Q2 | -1.0 | . 1 1947 | Q3 | -0.8 | . 2 1947 | Q4 | 6.4 | . 3 1948 | Q1 | 6.2 | . 4 1948 | Q2 | 6.8 | . When we plot this we will find that it looks very much like the graph in the article. We just have to play around the facet spacings to make it look continuous like a single bar chart instead of a faceted chart . alt.Chart(ann_gdp).mark_bar(width=5).encode( x=alt.X(&#39;quarter:O&#39;, title=None, axis=alt.Axis(labels=False, ticks=False)), y=&#39;ann_gdp:Q&#39;, facet=alt.Facet(&#39;year:O&#39;, bounds=&#39;flush&#39;, spacing={&#39;column&#39;:0}, header=alt.Header(labels=False, title=None)), #detail=&#39;quarter:N&#39;, color=alt.condition(alt.datum.ann_gdp &gt; 0, alt.value(&#39;green&#39;), alt.value(&#39;red&#39;)) ).configure_axis(grid=False).configure_view(strokeWidth=1, step=5) . Real GDP . The chart you see above is the annualized GDP which suggests that the US economy will shrink by a third if things stay exactly like the way they are now. Which is certainly not representative of current times. Fortunately the BEA also provides the Real GDP as Quantity Index units. If you apply the formaula to that data you get the data above i.e Table 1.1.1. Real GDP is a transformed version of nominal GDP Let&#39;s chart Table . Nominal GDP is reported as billions or trillions of dollars . re_gdp = (read_transpose_gdp_data(path=&#39;us_gdp.csv&#39;, if_uri=False) .pipe(clean_gdp_data) .pipe(rename_cols, col_dict={&#39;Gross domestic product&#39;: &#39;real_gdp&#39;})) re_gdp.head() . After function read_transpose_gdp_data ran, shape of dataframe is - (294, 28), execution time is - 0:00:00.066090 After function clean_gdp_data ran, shape of dataframe is - (294, 3), execution time is - 0:00:00.004825 After function rename_cols ran, shape of dataframe is - (294, 3), execution time is - 0:00:00.000353 . year quarter real_gdp . 0 1947 | Q1 | 12.552 | . 1 1947 | Q2 | 12.519 | . 2 1947 | Q3 | 12.493 | . 3 1947 | Q4 | 12.688 | . 4 1948 | Q1 | 12.879 | . Plotting this we will see how GDP has increased over the years - . alt.Chart(re_gdp).mark_bar(width=3.25).encode( x=alt.X(&#39;quarter:O&#39;, title=None, axis=alt.Axis(labels=False, ticks=False)), y=&#39;real_gdp:Q&#39;, column = alt.Facet(&#39;year:O&#39;, spacing=0, header=alt.Header(labels=False, title=None)) ).configure_view(strokeWidth=0, step=4) . Let&#39;s calculate non-annualized gdp from the real gdp - . usa_gdp = re_gdp.pipe(assign_columns) usa_gdp.head() . After function assign_columns ran, shape of dataframe is - (294, 4), execution time is - 0:00:00.003361 . year quarter real_gdp non_annualized_gdp . 0 1947 | Q1 | 12.552 | NaN | . 1 1947 | Q2 | 12.519 | -0.262906 | . 2 1947 | Q3 | 12.493 | -0.207684 | . 3 1947 | Q4 | 12.688 | 1.560874 | . 4 1948 | Q1 | 12.879 | 1.505359 | . Highlighting the quarters where we had negative growth compared to previous quarter (using non-annualized gdp) - . alt.Chart(usa_gdp).mark_bar(width=3.25).encode( x=alt.X(&#39;quarter:O&#39;, title=None, axis=alt.Axis(labels=False, ticks=False)), y=&#39;real_gdp:Q&#39;, column=alt.Facet(&#39;year:O&#39;, spacing=0, header=alt.Header(labels=False, title=None)), #detail=&#39;quarter:N&#39;, color=alt.condition(alt.datum.non_annualized_gdp &gt; 0, alt.value(&#39;#76a4a5&#39;), alt.value(&#39;#d0573a&#39;)) ).configure_axis(grid=True).configure_view(strokeWidth=0, step=4) . Finally let&#39;s plot the non-annualized GDP. We will see that it&#39;s no longer close to -30 but to -10, just like the chart in the article. . alt.Chart(usa_gdp).mark_bar(width=3.25).encode( x=alt.X(&#39;quarter:O&#39;, title=None, axis=alt.Axis(labels=False, ticks=False)), y=&#39;non_annualized_gdp:Q&#39;, column=alt.Facet(&#39;year:O&#39;, spacing=0, header=alt.Header(labels=False, title=None)), #detail=&#39;quarter:N&#39;, color=alt.condition(alt.datum.non_annualized_gdp &gt; 0, alt.value(&#39;#76a4a5&#39;), alt.value(&#39;#d0573a&#39;)) ).configure_axis(grid=False).configure_view(strokeWidth=0, step=4) . Let&#39;s add annualized gdp data to real and non-annualized gdp data to that we may plot them together for a bigger picture . usa_gdp = (usa_gdp .pipe(reshape_concat_column, ann_gdp[&#39;ann_gdp&#39;]) # because Annualized GDP and Real GDP have different shapes .pipe(year_as_time)) usa_gdp.head() . After function reshape_concat_column ran, shape of dataframe is - (293, 6), execution time is - 0:00:00.003945 . index year quarter real_gdp non_annualized_gdp annualized_gdp . 0 1 | 1947-01-01 | Q2 | 12.519 | -0.262906 | -1.0 | . 1 2 | 1947-01-01 | Q3 | 12.493 | -0.207684 | -0.8 | . 2 3 | 1947-01-01 | Q4 | 12.688 | 1.560874 | 6.4 | . 3 4 | 1948-01-01 | Q1 | 12.879 | 1.505359 | 6.2 | . 4 5 | 1948-01-01 | Q2 | 13.092 | 1.653855 | 6.8 | . To see the contrast b/w the two to understand how misleading annualized gdp can be we will layer them on top of each other - . a = alt.Chart().mark_bar(width=3.25).encode( x=alt.X(&#39;quarter:O&#39;, title=None, axis=alt.Axis(labels=False, domain=False, ticks=False)), y=&#39;non_annualized_gdp:Q&#39;, #column=alt.Facet(&#39;year:O&#39;, spacing=0, header=alt.Header(labels=False, title=None)), #detail=&#39;quarter:N&#39;, color=alt.condition(alt.datum.non_annualized_gdp &gt; 0, alt.value(&#39;blue&#39;), alt.value(&#39;maroon&#39;)), #text = &#39;num:Q&#39; ) b = alt.Chart().mark_bar(width=3.25).encode( x=alt.X(&#39;quarter:O&#39;, title=None, axis=alt.Axis(labels=False, ticks=False)), y=&#39;annualized_gdp:Q&#39;, #column=alt.Facet(&#39;year:O&#39;, spacing=0, header=alt.Header(labels=False, title=None)), #detail=&#39;quarter:N&#39;, color=alt.condition(alt.datum.annualized_gdp &gt; 0, alt.value(&#39;orange&#39;), alt.value(&#39;violet&#39;)), ) n_ann_gdp = alt.Chart().transform_filter({&#39;field&#39;: &#39;year&#39;, &#39;oneOf&#39;: [2008, 2020], &#39;timeUnit&#39;: &#39;year&#39;}).mark_text(color=&#39;maroon&#39;, dx=-12, dy=7, fontSize=12).encode( x=alt.X(&#39;quarter:O&#39;, title=None, aggregate={&#39;argmin&#39;: &#39;non_annualized_gdp&#39;}),# axis=alt.Axis(labels=False, domain=False, ticks=False)), y=&#39;min(non_annualized_gdp):Q&#39;, text=alt.Text(&#39;min(non_annualized_gdp):Q&#39;, format=&#39;.2&#39;, ) #x=alt.X(&#39;quarter&#39;, aggregate={&#39;argmin&#39;: &#39;non_annualized_gdp&#39;}) ) ann_gdp = alt.Chart().transform_filter(alt.FieldOneOfPredicate(field=&#39;year&#39;, oneOf=[2008, 2020], timeUnit=&#39;year&#39;)).mark_text(color=&#39;violet&#39;, dx=-12, dy=0, fontSize=12).encode( x=alt.X(&#39;quarter:O&#39;, title=None, aggregate={&#39;argmin&#39;: &#39;annualized_gdp&#39;}),# axis=alt.Axis(labels=False, domain=False, ticks=False)), y=alt.Y(&#39;min(annualized_gdp):Q&#39;, title=&#39;Annualized GDP, Real GDP&#39;), text=alt.Text(&#39;min(annualized_gdp):Q&#39;, format=&#39;.2&#39;, ) #x=alt.X(&#39;quarter&#39;, aggregate={&#39;argmin&#39;: &#39;non_annualized_gdp&#39;}) ) alt.layer(b, a, n_ann_gdp, ann_gdp, data=usa_gdp).facet( column=alt.Facet(&#39;year:T&#39;, header=alt.Header(labels=True, labelColor=&#39;grey&#39;, labelOrient=&#39;bottom&#39;, format=&quot;%y&quot;, formatType=&#39;time&#39;, title=None)), spacing=0, bounds=&#39;flush&#39; ).configure_axis(domain=False, labelColor=&#39;grey&#39;, tickColor=&#39;lightgrey&#39;, domainColor=&#39;lightgrey&#39;, titleColor=&#39;grey&#39; ).configure_view(strokeWidth=0, step=4).configure_axisY(grid=True,) . We can see clearly that the damage to the economy is greater than the 2008 recession .",
            "url": "https://armsp.github.io/covidviz/facet/nyt/gdp/2020/08/17/GDP.html",
            "relUrl": "/facet/nyt/gdp/2020/08/17/GDP.html",
            "date": " • Aug 17, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "US's failure to control the virus",
            "content": "Today we will make the first chart from the article The unique US failure to control the virus . . . . population_uri = &#39;https://github.com/CSSEGISandData/COVID-19/blob/master/csse_covid_19_data/UID_ISO_FIPS_LookUp_Table.csv?raw=true&#39; deaths_ts_uri = &#39;https://github.com/CSSEGISandData/COVID-19/blob/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv?raw=true&#39; gdp_current_us_dollars = &#39;https://gist.githubusercontent.com/armsp/58b43f28b4bf880f3874db80630dec44/raw/959a34a1797b0e3fdc860a6ef0057c62ee898dd7/gdp.csv&#39; . deaths_ts = pd.read_csv(deaths_ts_uri) deaths_ts.head() . Province/State Country/Region Lat Long 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 ... 7/31/20 8/1/20 8/2/20 8/3/20 8/4/20 8/5/20 8/6/20 8/7/20 8/8/20 8/9/20 . 0 NaN | Afghanistan | 33.93911 | 67.709953 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1272 | 1283 | 1284 | 1288 | 1288 | 1294 | 1298 | 1307 | 1312 | 1312 | . 1 NaN | Albania | 41.15330 | 20.168300 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 157 | 161 | 166 | 172 | 176 | 182 | 188 | 189 | 193 | 199 | . 2 NaN | Algeria | 28.03390 | 1.659600 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1210 | 1223 | 1231 | 1239 | 1248 | 1261 | 1273 | 1282 | 1293 | 1302 | . 3 NaN | Andorra | 42.50630 | 1.521800 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 52 | 52 | 52 | 52 | 52 | 52 | 52 | 52 | 52 | 52 | . 4 NaN | Angola | -11.20270 | 17.873900 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 52 | 54 | 55 | 58 | 59 | 62 | 64 | 67 | 70 | 75 | . 5 rows × 205 columns . gdp_us = pd.read_csv(gdp_current_us_dollars) gdp_us.head() . Series Name Series Code Country Name Country Code 2019 [YR2019] . 0 GDP per capita (current US$) | NY.GDP.PCAP.CD | Afghanistan | AFG | 502.115486913067 | . 1 GDP per capita (current US$) | NY.GDP.PCAP.CD | Albania | ALB | 5352.85741103671 | . 2 GDP per capita (current US$) | NY.GDP.PCAP.CD | Algeria | DZA | 3948.34327892571 | . 3 GDP per capita (current US$) | NY.GDP.PCAP.CD | American Samoa | ASM | .. | . 4 GDP per capita (current US$) | NY.GDP.PCAP.CD | Andorra | AND | 40886.3911648431 | . population = pd.read_csv(population_uri) population.head() . UID iso2 iso3 code3 FIPS Admin2 Province_State Country_Region Lat Long_ Combined_Key Population . 0 4 | AF | AFG | 4.0 | NaN | NaN | NaN | Afghanistan | 33.93911 | 67.709953 | Afghanistan | 38928341.0 | . 1 8 | AL | ALB | 8.0 | NaN | NaN | NaN | Albania | 41.15330 | 20.168300 | Albania | 2877800.0 | . 2 12 | DZ | DZA | 12.0 | NaN | NaN | NaN | Algeria | 28.03390 | 1.659600 | Algeria | 43851043.0 | . 3 20 | AD | AND | 20.0 | NaN | NaN | NaN | Andorra | 42.50630 | 1.521800 | Andorra | 77265.0 | . 4 24 | AO | AGO | 24.0 | NaN | NaN | NaN | Angola | -11.20270 | 17.873900 | Angola | 32866268.0 | . import pandas as pd import altair as alt from functools import wraps import datetime as dt def log_step(func): @wraps(func) def wrapper(dataf, *args, **kwargs): &quot;&quot;&quot;timing or logging etc&quot;&quot;&quot; start = dt.datetime.now() output = func(dataf, *args, **kwargs) end = dt.datetime.now() print(f&quot;After function {func.__name__} ran, shape of dataframe is - {output.shape}, execution time is - {end-start}&quot;) return output return wrapper @log_step def start_pipeline(dataf): return dataf.copy() @log_step def remove_cols(dataf, *arg, **kwargs): #print(list(arg)) result = dataf.drop(columns=list(arg)) return result @log_step def remove_null(dataf): return dataf.dropna() def rename_cols(dataf, *arg, **kwargs): &quot;&quot;&quot;Rename column names of raw dataframes to something digestable and that looks better in visualization and does not have spaces in between cause altair does not like that&quot;&quot;&quot; result = dataf.rename(columns=kwargs) return result @log_step def filter_rows(dataf, which, **kwargs): if which == &#39;gdp&#39;: result = dataf[dataf[&#39;current_us&#39;] != &#39;..&#39;] return result elif which == &#39;pop&#39;: result = dataf[pd.isnull(dataf[&#39;Province_State&#39;])] return result def set_dtypes(dataf): &quot;&quot;&quot;set the datatypes of columns&quot;&quot;&quot; # can use data.assign(col = lambda d: pd.to_datetime(d[&#39;col&#39;])) or col = pd.to_datetime(d[&#39;col&#39;]) dataf[&#39;current_us&#39;] = dataf[&#39;current_us&#39;].astype(float) return dataf # def remove_outliers(dataf): # &quot;&quot;&quot;remove outliers&quot;&quot;&quot; # return dataf # def add_features(dataf): # return dataf @log_step def clean(dataf): agg_deaths = dataf.groupby(&#39;Country/Region&#39;).sum().reset_index() agg_deaths = agg_deaths[agg_deaths[&#39;Country/Region&#39;].isin(pop_w_gdp[&#39;Country/Region&#39;])].set_index(&#39;Country/Region&#39;) result = agg_deaths.T.reset_index().rename_axis(None, axis=1).rename(columns={&#39;index&#39;: &#39;Date&#39;}) #convert cumulative deaths to daily deaths per million for col in result: if col != &#39;Date&#39;: result[col] = result[col].diff() result[col] = (result[col]/int(countries_population[countries_population[&#39;Country/Region&#39;] == col][&#39;Population&#39;]))*1000000 return result . gdp = (gdp_us .pipe(start_pipeline) .pipe(remove_null) .pipe(remove_cols, *[&#39;Series Name&#39;, &#39;Series Code&#39;]) .pipe(rename_cols, **{&#39;2019 [YR2019]&#39;: &#39;current_us&#39;}) .pipe(filter_rows, &#39;gdp&#39;) .pipe(set_dtypes)) countries_population = (population .pipe(start_pipeline) .pipe(filter_rows, &#39;pop&#39;) .pipe(remove_cols, *[&#39;UID&#39;, &#39;iso2&#39;, &#39;code3&#39;, &#39;FIPS&#39;, &#39;Admin2&#39;, &#39;Province_State&#39;, &#39;Lat&#39;, &#39;Long_&#39;, &#39;Combined_Key&#39;]) .pipe(rename_cols, **{&#39;iso3&#39;: &#39;Country Code&#39;, &#39;Country_Region&#39;:&#39;Country/Region&#39;})) # Combining population with GDP pop_w_gdp = countries_population.merge(gdp, how=&#39;inner&#39;, on=&#39;Country Code&#39;) # Filter for only wealthy countries i.e GDP &gt; 25000 USD and population &gt; 10 million pop_w_gdp = pop_w_gdp[(pop_w_gdp[&#39;current_us&#39;] &gt; 25000) &amp; (pop_w_gdp[&#39;Population&#39;] &gt; 10000000)] # Making daily deaths per million data for plotting plot_data = (deaths_ts .pipe(start_pipeline) .pipe(remove_cols, *[&#39;Province/State&#39;, &#39;Lat&#39;, &#39;Long&#39;]) .pipe(clean) .pipe(remove_null) ) plot_data . After function start_pipeline ran, shape of dataframe is - (269, 5), execution time is - 0:00:00.000104 After function remove_null ran, shape of dataframe is - (264, 5), execution time is - 0:00:00.001648 After function remove_cols ran, shape of dataframe is - (264, 3), execution time is - 0:00:00.001438 After function filter_rows ran, shape of dataframe is - (222, 3), execution time is - 0:00:00.000781 After function start_pipeline ran, shape of dataframe is - (4153, 12), execution time is - 0:00:00.000478 After function filter_rows ran, shape of dataframe is - (188, 12), execution time is - 0:00:00.001278 After function remove_cols ran, shape of dataframe is - (188, 3), execution time is - 0:00:00.000900 After function start_pipeline ran, shape of dataframe is - (266, 205), execution time is - 0:00:00.000276 After function remove_cols ran, shape of dataframe is - (266, 202), execution time is - 0:00:00.001868 After function clean ran, shape of dataframe is - (201, 14), execution time is - 0:00:00.028541 After function remove_null ran, shape of dataframe is - (200, 14), execution time is - 0:00:00.002357 . Date Australia Belgium Canada France Germany Italy Japan Korea, South Netherlands Spain Sweden US United Kingdom . 1 1/23/20 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 2 1/24/20 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 3 1/25/20 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 4 1/26/20 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 5 1/27/20 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 196 8/5/20 | 0.314222 | 0.603989 | 0.132080 | 0.000000 | 0.190967 | 0.165394 | 0.039533 | 0.000000 | 0.175082 | 0.021388 | 1.287222 | 4.164311 | 1.988628 | . 197 8/6/20 | 0.432055 | 0.172568 | 0.079248 | 0.168522 | 0.023871 | 0.099236 | 0.047440 | 0.019505 | 0.000000 | 0.021388 | 0.594102 | 3.794015 | 0.721798 | . 198 8/7/20 | 0.471333 | 0.431421 | 0.105664 | 0.291083 | 0.167096 | 0.049618 | 0.063253 | 0.019505 | 0.058361 | 0.064165 | -0.297051 | 3.772768 | 1.443597 | . 199 8/8/20 | 0.667722 | 0.345137 | 0.184913 | 0.000000 | 0.071613 | 0.215012 | 0.000000 | 0.019505 | 0.233442 | 0.000000 | 0.000000 | 3.265888 | 0.810182 | . 200 8/9/20 | 0.707000 | 0.172568 | 0.105664 | 0.000000 | 0.011935 | 0.033079 | 0.039533 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.563134 | 0.117845 | . 200 rows × 14 columns . highlight = alt.selection_single(on=&#39;mouseover&#39;, empty=&#39;none&#39;, fields=[&#39;key&#39;], nearest=True, init={&#39;key&#39;: &#39;US&#39;}) base = (alt.Chart(plot_data, title=&quot;Daily Deaths per million in wealthy countries&quot;) .transform_fold(list(plot_data.columns[1:])) .transform_window(deaths=&#39;mean(value)&#39;, frame=[-6,0], groupby=[&#39;key&#39;]) .encode( x=alt.X(&#39;Date:T&#39;, axis=alt.Axis(format=&quot;%b&quot;, tickCount=7), title=None), y=alt.Y(&#39;deaths:Q&#39;, axis=alt.Axis(tickCount=8), title=None), tooltip=[&#39;key:N&#39;] ).properties(height=600)) points = base.mark_circle().encode( opacity=alt.value(0) ).add_selection(highlight).properties( width=600 ) line = base.mark_line(stroke=&#39;grey&#39;).encode( #x=&#39;Date:T&#39;, #y=&#39;deaths:Q&#39;, #color=&#39;key:N&#39;, #color=alt.condition(alt.datum.key == &#39;US&#39;, alt.value(&#39;red&#39;), alt.value(&#39;grey&#39;)), detail=&#39;key:N&#39; #size=alt.condition(highlight, alt.value(5), alt.value(2)), ) top_layer = (base.transform_filter(highlight).mark_line() .encode( color=alt.condition(alt.datum.key == &#39;US&#39;, alt.value(&#39;red&#39;), alt.value(&#39;grey&#39;)), size=alt.value(4) )) usa_layer = (base.transform_filter(alt.datum.key == &#39;US&#39;).mark_line() .encode( color=alt.value(&#39;red&#39;), )) circles = base.mark_circle(size=100, filled=True, strokeOpacity=0.7, fillOpacity=0.3, dx=-20).encode( y=alt.Y(&#39;deaths:Q&#39;, aggregate={&#39;argmax&#39;: &#39;Date&#39;}), x=alt.X(&#39;Date:T&#39;, aggregate=&#39;max&#39;), stroke=alt.condition(alt.datum.key==&#39;US&#39;, alt.value(&#39;red&#39;), alt.value(&#39;grey&#39;)), color=alt.condition(alt.datum.key==&#39;US&#39;, alt.value(&#39;red&#39;), alt.value(&#39;grey&#39;)), ) text = circles.transform_filter({&#39;field&#39;: &#39;key&#39;, &#39;oneOf&#39;: [&#39;US&#39;, &#39;United Kingdom&#39;, &#39;Korea, South&#39;]}).mark_text(dx=10, align=&#39;left&#39;).encode( text=&#39;key:N&#39; ) (points+line+top_layer+usa_layer+circles+text).configure_view(strokeWidth=0).configure_axis(grid=True) .",
            "url": "https://armsp.github.io/covidviz/interactive,/nyt/2020/08/10/US_failed.html",
            "relUrl": "/interactive,/nyt/2020/08/10/US_failed.html",
            "date": " • Aug 10, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Changes in spending",
            "content": "Today we will study the charts in the article The Rich Cut Their Spending. That Has Hurt All the Workers Who Count on It. These charts tell us something very important about how the spending has been cut differently across different classes. . . . Note: The vertical lines correspond to the following dates - . First stimulus checks - April 17 | States in the process of reopening - May 1 | . The data for this analysis is taken from Opportunity Labs where they publish their data in this dashboard. . What&#39;s important about this data is best summed up by - . One of the things this crisis has made salient is how interdependent our health was, said Michael Stepner, an economist at the University of Toronto. We’re seeing the mirror of that on the economic side. . #hide_output import pandas as pd import altair as alt alt.renderers.set_embed_options(actions=False) . Drop in consumer spending . The rich drive more of the economy than they did 50 years ago. And more workers depend on them. . For the highest-income quarter, spending has recovered much more slowly, after falling by 36 percent at the lowest point. . . Important: We will use data till July only, so the uri used for the data is for the commit of a particular day. If you want to use the latest data then replace the spending_uri with this - &#8217;https://raw.githubusercontent.com/OpportunityInsights/EconomicTracker/main/data/Affinity%20-%20National%20-%20Daily.csv&amp;#8217; . spending_uri = &#39;https://raw.githubusercontent.com/OpportunityInsights/EconomicTracker/8d9fae46fab3e386a8f4ce798de09a016cbda0f9/data/Affinity%20-%20National%20-%20Daily.csv&#39; #spending_uri = &#39;https://raw.githubusercontent.com/Opportunitylab/EconomicTracker/main/data/Affinity%20-%20National%20-%20Weekly.csv&#39; # for latest data spending = pd.read_csv(spending_uri) spending.head() . year month day spend_acf spend_aer spend_all spend_all_inchigh spend_all_inclow spend_all_incmiddle spend_apg spend_grf spend_hcs spend_tws . 0 2020 | 1 | 24 | -0.00510 | -0.02360 | -0.006440 | -0.005790 | -0.00752 | -0.00654 | -0.00952 | -0.00954 | -0.00328 | -0.005840 | . 1 2020 | 1 | 25 | 0.00202 | -0.01820 | 0.000432 | -0.000625 | -0.00201 | 0.00199 | 0.00400 | 0.00991 | -0.00469 | 0.000839 | . 2 2020 | 1 | 26 | -0.00896 | -0.02220 | -0.002710 | -0.000425 | -0.00668 | -0.00315 | 0.00152 | 0.01920 | -0.00647 | 0.002720 | . 3 2020 | 1 | 27 | -0.01350 | -0.00762 | -0.012200 | -0.011000 | -0.01590 | -0.01190 | -0.00671 | -0.00980 | -0.00755 | -0.015700 | . 4 2020 | 1 | 28 | -0.01550 | -0.01270 | -0.013700 | -0.013300 | -0.01630 | -0.01320 | -0.00492 | -0.01820 | -0.00243 | -0.009870 | . def add_format_date(df): df[&#39;date&#39;] = df[&#39;year&#39;].astype(str) + &#39;-&#39; + df[&#39;month&#39;].astype(str) + &#39;-&#39; + df[&#39;day&#39;].astype(str) df[&#39;date&#39;] = pd.to_datetime(df[&#39;date&#39;], format=&quot;%Y-%m-%d&quot;) return df . spending = spending.pipe(add_format_date) spending.head() . year month day spend_acf spend_aer spend_all spend_all_inchigh spend_all_inclow spend_all_incmiddle spend_apg spend_grf spend_hcs spend_tws date . 0 2020 | 1 | 24 | -0.00510 | -0.02360 | -0.006440 | -0.005790 | -0.00752 | -0.00654 | -0.00952 | -0.00954 | -0.00328 | -0.005840 | 2020-01-24 | . 1 2020 | 1 | 25 | 0.00202 | -0.01820 | 0.000432 | -0.000625 | -0.00201 | 0.00199 | 0.00400 | 0.00991 | -0.00469 | 0.000839 | 2020-01-25 | . 2 2020 | 1 | 26 | -0.00896 | -0.02220 | -0.002710 | -0.000425 | -0.00668 | -0.00315 | 0.00152 | 0.01920 | -0.00647 | 0.002720 | 2020-01-26 | . 3 2020 | 1 | 27 | -0.01350 | -0.00762 | -0.012200 | -0.011000 | -0.01590 | -0.01190 | -0.00671 | -0.00980 | -0.00755 | -0.015700 | 2020-01-27 | . 4 2020 | 1 | 28 | -0.01550 | -0.01270 | -0.013700 | -0.013300 | -0.01630 | -0.01320 | -0.00492 | -0.01820 | -0.00243 | -0.009870 | 2020-01-28 | . Plotting the data - . base=alt.Chart(spending).transform_fold([&#39;spend_all_inchigh&#39;, &#39;spend_all_inclow&#39;, &#39;spend_all_incmiddle&#39;]).transform_filter(alt.datum.date &gt; alt.expr.toDate(&#39;2020-02-14&#39;)).mark_line().encode( x=alt.X(&#39;date:T&#39;, title=None, axis=alt.Axis(format=&quot;%b%e&quot;, tickCount=5, labelOffset=0, tickOffset=0, labelPadding=25, ticks=False)), #y=&#39;spend_all_inchigh:Q&#39;, #x2=&#39;date:Q&#39;, y=alt.Y(&#39;value:Q&#39;, title=None, axis=alt.Axis(format=&quot;%&quot;, tickCount=10)), color=&#39;key:N&#39; #detail=&#39;date&#39; ).properties(width=900, height=600) lines={&#39;lines&#39;: [&#39;2020-04-15&#39;, &#39;2020-05-01&#39;], &#39;y1&#39;: [0,0], &#39;y2&#39;: [-0.4, -0.4]} lines1={&#39;lines&#39;: [&#39;2020-04-15&#39;], &#39;text&#39;: [&#39;First stimulus n checks received&#39;], &#39;y&#39;: [-0.03]} lines2={&#39;lines&#39;: [&#39;2020-05-01&#39;], &#39;text&#39;: [&#39;Half of states in n process of reopening&#39;], &#39;y&#39;: [-0.03]} vert_line = alt.Chart(pd.DataFrame(lines)).mark_rule(strokeDash=[5,5], stroke=&#39;grey&#39;).encode( x=&#39;lines:T&#39;, y=alt.Y(&#39;y1:Q&#39;, scale=alt.Scale(zero=False)), #y2=alt.Y2(&#39;y2:Q&#39;) ) text1 = alt.Chart(pd.DataFrame(lines1)).mark_text(lineBreak=&#39; n&#39;, dx=-10, align=&#39;right&#39;).encode( text = &#39;text:N&#39;, y = &#39;y:Q&#39;, x = &#39;lines:T&#39; ) text2 = alt.Chart(pd.DataFrame(lines2)).mark_text(lineBreak=&#39; n&#39;,dx=10, align=&#39;left&#39;).encode( text = &#39;text:N&#39;, y = &#39;y:Q&#39;, x = &#39;lines:T&#39;, ) alt.layer(base, vert_line, text1, text2).configure_view(strokeWidth=0).configure_axis(grid=False).configure_axisX(orient=&#39;top&#39;, offset=-67) . We can use the same techniques for the vertical lines and text overlay as the chart above in the following charts. Since the idea is similar I will not implement them for all, instead just plot the line charts. . Small businesses in the richest neighborhoods have had the biggest drops in revenue . . Note: Latest data from now on . revenue_uri = &#39;https://raw.githubusercontent.com/Opportunitylab/EconomicTracker/main/data/Womply%20Revenue%20-%20National%20-%20Daily.csv&#39; revenue = pd.read_csv(revenue_uri) revenue = revenue.pipe(add_format_date) revenue.head() . year month day revenue_all revenue_inchigh revenue_inclow revenue_incmiddle revenue_ss40 revenue_ss65 revenue_ss70 date . 0 2020 | 1 | 10 | -0.01170 | -0.00490 | -0.0274 | -0.00860 | -0.00999 | -0.03060 | -0.0162 | 2020-01-10 | . 1 2020 | 1 | 11 | -0.00348 | 0.00729 | -0.0222 | -0.00302 | -0.00692 | -0.00958 | -0.0138 | 2020-01-11 | . 2 2020 | 1 | 12 | 0.00195 | 0.00998 | -0.0175 | 0.00372 | -0.00488 | 0.00284 | -0.0117 | 2020-01-12 | . 3 2020 | 1 | 13 | -0.01350 | -0.00551 | -0.0481 | -0.00665 | -0.00459 | -0.04560 | -0.0217 | 2020-01-13 | . 4 2020 | 1 | 14 | -0.00138 | 0.00071 | -0.0231 | 0.00392 | -0.00174 | 0.00809 | -0.0168 | 2020-01-14 | . alt.Chart(revenue).mark_line().transform_fold([&#39;revenue_inclow&#39;, &#39;revenue_incmiddle&#39;, &#39;revenue_inchigh&#39;]).transform_filter(alt.datum.date &gt; alt.expr.toDate(&#39;2020-02-14&#39;)).encode( x=&#39;date:T&#39;, y= &#39;value:Q&#39;, color= &#39;key:N&#39; ) . Low-wage workers in the richest neighborhoods have had the biggest drop in earnings . . Important: The file for this data has been removed and not updated since July. So we will use the data from the particular commit that had this file. . earning_uri = &#39;https://raw.githubusercontent.com/OpportunityInsights/EconomicTracker/5f914ee4e71f56a33857b63e0bd07d71bc31e847/data/Low%20Inc%20Earnings%20Small%20Businesses%20-%20National%20-%20Daily.csv&#39; earning = pd.read_csv(earning_uri) earning = earning.pipe(add_format_date) earning.head() . year month day pay pay31_33 pay44_45 pay48_49 pay62 pay72 pay_inclow pay_incmiddle pay_inchigh date . 0 2020 | 1 | 1 | . | . | . | . | . | . | . | . | . | 2020-01-01 | . 1 2020 | 1 | 2 | . | . | . | . | . | . | . | . | . | 2020-01-02 | . 2 2020 | 1 | 3 | . | . | . | . | . | . | . | . | . | 2020-01-03 | . 3 2020 | 1 | 4 | . | . | . | . | . | . | . | . | . | 2020-01-04 | . 4 2020 | 1 | 5 | . | . | . | . | . | . | . | . | . | 2020-01-05 | . alt.Chart(earning).mark_line().transform_fold([&#39;pay&#39;, &#39;pay_inclow&#39;, &#39;pay_incmiddle&#39;, &#39;pay_inchigh&#39;]).encode( x=&#39;date:T&#39;, y= &#39;value:Q&#39;, color= &#39;key:N&#39; ) . Low-wage workers in the richest neighborhoods have had the biggest drop in employment . . Important: This file was also eventually removed. So we will use the file from the commit that still had this file- . #employment_uri = &#39;https://raw.githubusercontent.com/Opportunitylab/EconomicTracker/main/data/Low%20Inc%20Emp%20Small%20Businesses%20-%20National%20-%20Daily.csv&#39; # original file employment_uri = &#39;https://raw.githubusercontent.com/OpportunityInsights/EconomicTracker/ba8c0096efb873d90f10cd720576c4ec5e6fc42e/data/Low%20Inc%20Emp%20Small%20Businesses%20-%20National%20-%20Daily.csv&#39; employment = pd.read_csv(employment_uri) employment = employment.pipe(add_format_date) employment.head() . year month day emp emp31_33 emp44_45 emp48_49 emp62 emp72 emp_inclow emp_incmiddle emp_inchigh date . 0 2020 | 1 | 1 | . | . | . | . | . | . | . | . | . | 2020-01-01 | . 1 2020 | 1 | 2 | . | . | . | . | . | . | . | . | . | 2020-01-02 | . 2 2020 | 1 | 3 | . | . | . | . | . | . | . | . | . | 2020-01-03 | . 3 2020 | 1 | 4 | . | . | . | . | . | . | . | . | . | 2020-01-04 | . 4 2020 | 1 | 5 | . | . | . | . | . | . | . | . | . | 2020-01-05 | . alt.Chart(employment).mark_line().transform_fold([&#39;emp_inclow&#39;, &#39;emp_incmiddle&#39;, &#39;emp_inchigh&#39;]).encode( x=&#39;date:T&#39;, y= &#39;value:Q&#39;, color= &#39;key:N&#39; ) .",
            "url": "https://armsp.github.io/covidviz/spending/nyt/2020/07/15/Spending.html",
            "relUrl": "/spending/nyt/2020/07/15/Spending.html",
            "date": " • Jul 15, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Latest epicenters of infection and death toll in the US",
            "content": "Today we will make a very special graph that looks like the article See How the Coronavirus Death Toll Grew Across the U.S. . . This will require us to use some special techniques using SVG Path elements. . import geopandas as gpd import altair as alt import pandas as pd alt.renderers.set_embed_options(actions=True) # NYT dataset county_url = &#39;https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv&#39; cdf = pd.read_csv(county_url) . cdf.head() . date county state fips cases deaths . 0 2020-01-21 | Snohomish | Washington | 53061.0 | 1 | 0 | . 1 2020-01-22 | Snohomish | Washington | 53061.0 | 1 | 0 | . 2 2020-01-23 | Snohomish | Washington | 53061.0 | 1 | 0 | . 3 2020-01-24 | Cook | Illinois | 17031.0 | 1 | 0 | . 4 2020-01-24 | Snohomish | Washington | 53061.0 | 1 | 0 | . # Shapefiles from us census state_shpfile = &#39;./shapes/cb_2019_us_state_20m&#39; county_shpfile = &#39;./shapes/cb_2019_us_county_20m&#39; states = gpd.read_file(state_shpfile) county = gpd.read_file(county_shpfile) # Adding longitude and latitude in state data states[&#39;lon&#39;] = states[&#39;geometry&#39;].centroid.x states[&#39;lat&#39;] = states[&#39;geometry&#39;].centroid.y # Adding longitude and latitude in county data county[&#39;lon&#39;] = county[&#39;geometry&#39;].centroid.x county[&#39;lat&#39;] = county[&#39;geometry&#39;].centroid.y . NYT publishes the data for New York City in a different way. So we will add custom FIPS for New York City and Puerto Rico too whose county level information is not present. . cdf.loc[cdf[&#39;county&#39;] == &#39;New York City&#39;,&#39;fips&#39;] = 1 cdf[cdf[&#39;county&#39;] == &#39;New York City&#39;].head() . date county state fips cases deaths . 416 2020-03-01 | New York City | New York | 1.0 | 1 | 0 | . 448 2020-03-02 | New York City | New York | 1.0 | 1 | 0 | . 482 2020-03-03 | New York City | New York | 1.0 | 2 | 0 | . 518 2020-03-04 | New York City | New York | 1.0 | 2 | 0 | . 565 2020-03-05 | New York City | New York | 1.0 | 4 | 0 | . cdf.loc[cdf[&#39;state&#39;] == &#39;Puerto Rico&#39;, &#39;fips&#39;] = 2 cdf[cdf[&#39;state&#39;] == &#39;Puerto Rico&#39;].head() . date county state fips cases deaths . 1858 2020-03-13 | Unknown | Puerto Rico | 2.0 | 3 | 0 | . 2220 2020-03-14 | Unknown | Puerto Rico | 2.0 | 4 | 0 | . 2642 2020-03-15 | Unknown | Puerto Rico | 2.0 | 5 | 0 | . 3107 2020-03-16 | Unknown | Puerto Rico | 2.0 | 5 | 0 | . 3637 2020-03-17 | Unknown | Puerto Rico | 2.0 | 5 | 0 | . Extracting the latest cases and deaths - . aggregate = cdf.groupby(&#39;fips&#39;, as_index=False).agg({&#39;county&#39;: &#39;first&#39;, &#39;date&#39;: &#39;last&#39;, &#39;state&#39;: &#39;last&#39;, &#39;cases&#39;: &#39;last&#39;, &#39;deaths&#39;: &#39;last&#39;}) aggregate.head() . fips county date state cases deaths . 0 1.0 | New York City | 2020-07-04 | New York | 221395 | 22630 | . 1 2.0 | Unknown | 2020-07-04 | Puerto Rico | 7787 | 155 | . 2 1001.0 | Autauga | 2020-07-04 | Alabama | 591 | 13 | . 3 1003.0 | Baldwin | 2020-07-04 | Alabama | 863 | 10 | . 4 1005.0 | Barbour | 2020-07-04 | Alabama | 350 | 2 | . Combining the 5 boroughs - New York, Kings, Queens, Bronx and Richmond - into one and adding that spatial area in the geodatatrame . #New York City fips = 36005&#39;, &#39;36047&#39;, &#39;36061&#39;, &#39;36081&#39;, &#39;36085 which corresponds to New York, Kings, Queens, Bronx and Richmond spatial_nyc = county[county[&#39;GEOID&#39;].isin([&#39;36005&#39;, &#39;36047&#39;, &#39;36061&#39;, &#39;36081&#39;, &#39;36085&#39;])] . combined_nyc = spatial_nyc.dissolve(by=&#39;STATEFP&#39;) alt.Chart(spatial_nyc).mark_geoshape(stroke=&#39;white&#39;, strokeWidth=3).encode() | alt.Chart(combined_nyc).mark_geoshape(stroke=&#39;white&#39;, strokeWidth=3).encode() . agg_nyc_data = spatial_nyc.dissolve(by=&#39;STATEFP&#39;).reset_index() agg_nyc_data[&#39;GEOID&#39;] = &#39;1&#39; agg_nyc_data[&#39;fips&#39;] = 1 agg_nyc_data[&#39;lon&#39;] = agg_nyc_data[&#39;geometry&#39;].centroid.x agg_nyc_data[&#39;lat&#39;] = agg_nyc_data[&#39;geometry&#39;].centroid.y . county = gpd.GeoDataFrame(pd.concat([county, agg_nyc_data], ignore_index=True)) county[&#39;fips&#39;] = county[&#39;GEOID&#39;] county[&#39;fips&#39;] = county[&#39;fips&#39;].astype(&#39;int&#39;) county.head() . STATEFP COUNTYFP COUNTYNS AFFGEOID GEOID NAME LSAD ALAND AWATER geometry lon lat fips . 0 29 | 227 | 00758566 | 0500000US29227 | 29227 | Worth | 06 | 690564983 | 493903 | POLYGON ((-94.63203 40.57176, -94.53388 40.570... | -94.423288 | 40.479456 | 29227 | . 1 31 | 061 | 00835852 | 0500000US31061 | 31061 | Franklin | 06 | 1491355860 | 487899 | POLYGON ((-99.17940 40.35068, -98.72683 40.350... | -98.952991 | 40.176363 | 31061 | . 2 36 | 013 | 00974105 | 0500000US36013 | 36013 | Chautauqua | 06 | 2746047476 | 1139407865 | POLYGON ((-79.76195 42.26986, -79.62748 42.324... | -79.366918 | 42.227692 | 36013 | . 3 37 | 181 | 01008591 | 0500000US37181 | 37181 | Vance | 06 | 653713542 | 42178610 | POLYGON ((-78.49773 36.51467, -78.45728 36.541... | -78.406712 | 36.368814 | 37181 | . 4 47 | 183 | 01639799 | 0500000US47183 | 47183 | Weakley | 06 | 1503107848 | 3707114 | POLYGON ((-88.94916 36.41010, -88.81642 36.410... | -88.719909 | 36.298962 | 47183 | . We will actually work with Metropolitan Statistical Areas instead of counties, so we need more work to do. We have the MSA Shapefile as well as a dataset that has the counties that combine to form MSAs from the US Census . msa = pd.read_csv(&#39;core_msa_list.csv&#39;, sep=&quot;;&quot;) msa_shp = gpd.read_file(&#39;shapes/cb_2019_us_cbsa_500k/cb_2019_us_cbsa_500k.shp&#39;) #msa[msa[&#39;CBSA Title&#39;].str.startswith(&#39;New York&#39;)] . msa.head() . CBSA Code Metropolitan Division Code CSA Code CBSA Title Metropolitan/Micropolitan Statistical Area Metropolitan Division Title CSA Title County/County Equivalent State Name FIPS State Code FIPS County Code Central/Outlying County . 0 10100 | NaN | NaN | Aberdeen, SD | Micropolitan Statistical Area | NaN | NaN | Brown County | South Dakota | 46 | 13 | Central | . 1 10100 | NaN | NaN | Aberdeen, SD | Micropolitan Statistical Area | NaN | NaN | Edmunds County | South Dakota | 46 | 45 | Outlying | . 2 10140 | NaN | NaN | Aberdeen, WA | Micropolitan Statistical Area | NaN | NaN | Grays Harbor County | Washington | 53 | 27 | Central | . 3 10180 | NaN | NaN | Abilene, TX | Metropolitan Statistical Area | NaN | NaN | Callahan County | Texas | 48 | 59 | Outlying | . 4 10180 | NaN | NaN | Abilene, TX | Metropolitan Statistical Area | NaN | NaN | Jones County | Texas | 48 | 253 | Outlying | . msa_shp.head() . CSAFP CBSAFP AFFGEOID GEOID NAME LSAD ALAND AWATER geometry . 0 425 | 37620 | 310M500US37620 | 37620 | Parkersburg-Vienna, WV | M1 | 1551452495 | 32408833 | POLYGON ((-81.75582 39.18052, -81.75575 39.180... | . 1 None | 45980 | 310M500US45980 | 45980 | Troy, AL | M2 | 1740647520 | 2336975 | POLYGON ((-86.19941 31.80786, -86.19808 31.808... | . 2 548 | 49020 | 310M500US49020 | 49020 | Winchester, VA-WV | M1 | 2752545068 | 16892497 | POLYGON ((-78.97849 39.23900, -78.97626 39.243... | . 3 142 | 45180 | 310M500US45180 | 45180 | Talladega-Sylacauga, AL | M2 | 1908293036 | 60927931 | POLYGON ((-86.50359 33.17598, -86.50313 33.179... | . 4 None | 25060 | 310M500US25060 | 25060 | Gulfport-Biloxi, MS | M1 | 5739122781 | 2105780374 | MULTIPOLYGON (((-88.50297 30.21523, -88.49176 ... | . msa[&#39;FIPS State Code&#39;] = msa[&#39;FIPS State Code&#39;].astype(str) msa[&#39;FIPS County Code&#39;] = msa[&#39;FIPS County Code&#39;].astype(str) . state_fips_max_length = msa[&#39;FIPS State Code&#39;].map(len).max() county_fips_max_length = msa[&#39;FIPS County Code&#39;].map(len).max() . msa[&#39;FIPS State Code&#39;] = msa[&#39;FIPS State Code&#39;].apply(lambda x: &#39;0&#39;*(state_fips_max_length - len(x))+x) msa[&#39;FIPS County Code&#39;] = msa[&#39;FIPS County Code&#39;].apply(lambda x: &#39;0&#39;*(county_fips_max_length - len(x))+x) . msa[&#39;fips&#39;] = msa[&#39;FIPS State Code&#39;]+msa[&#39;FIPS County Code&#39;] msa[&#39;fips&#39;] = msa[&#39;fips&#39;].astype(float) . Now we will add a row for New York . nyc_temp = pd.DataFrame({&#39;CBSA Code&#39;: 35620,&#39;Metropolitan Division Code&#39;: None, &#39;CSA Code&#39;: 408, &#39;CBSA Title&#39;: None,&#39;Metropolitan/Micropolitan Statistical Area&#39;: None, &#39;Metropolitan Division Title&#39;: None,&#39;CSA Title&#39;: None,&#39;County/County Equivalent&#39;: None, &#39;State Name&#39;: &#39;New York&#39;, &#39;FIPS State Code&#39;: 36, &#39;FIPS County Code&#39;: None, &#39;Central/Outlying County&#39;: None, &#39;fips&#39;: 1},index=[0]) msa = pd.concat([msa, nyc_temp], ignore_index=True) msa . CBSA Code Metropolitan Division Code CSA Code CBSA Title Metropolitan/Micropolitan Statistical Area Metropolitan Division Title CSA Title County/County Equivalent State Name FIPS State Code FIPS County Code Central/Outlying County fips . 0 10100 | NaN | NaN | Aberdeen, SD | Micropolitan Statistical Area | NaN | NaN | Brown County | South Dakota | 46 | 013 | Central | 46013.0 | . 1 10100 | NaN | NaN | Aberdeen, SD | Micropolitan Statistical Area | NaN | NaN | Edmunds County | South Dakota | 46 | 045 | Outlying | 46045.0 | . 2 10140 | NaN | NaN | Aberdeen, WA | Micropolitan Statistical Area | NaN | NaN | Grays Harbor County | Washington | 53 | 027 | Central | 53027.0 | . 3 10180 | NaN | NaN | Abilene, TX | Metropolitan Statistical Area | NaN | NaN | Callahan County | Texas | 48 | 059 | Outlying | 48059.0 | . 4 10180 | NaN | NaN | Abilene, TX | Metropolitan Statistical Area | NaN | NaN | Jones County | Texas | 48 | 253 | Outlying | 48253.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 1912 49700 | NaN | 472.0 | Yuba City, CA | Metropolitan Statistical Area | NaN | Sacramento-Roseville, CA | Yuba County | California | 06 | 115 | Central | 6115.0 | . 1913 49740 | NaN | NaN | Yuma, AZ | Metropolitan Statistical Area | NaN | NaN | Yuma County | Arizona | 04 | 027 | Central | 4027.0 | . 1914 49780 | NaN | 198.0 | Zanesville, OH | Micropolitan Statistical Area | NaN | Columbus-Marion-Zanesville, OH | Muskingum County | Ohio | 39 | 119 | Central | 39119.0 | . 1915 49820 | NaN | NaN | Zapata, TX | Micropolitan Statistical Area | NaN | NaN | Zapata County | Texas | 48 | 505 | Central | 48505.0 | . 1916 35620 | None | 408.0 | None | None | None | None | None | New York | 36 | None | None | 1.0 | . 1917 rows × 13 columns . msa[&#39;CBSA Code&#39;] = msa[&#39;CBSA Code&#39;].astype(float) . msa[msa[&#39;fips&#39;].isin(aggregate[&#39;fips&#39;]) == False] . CBSA Code Metropolitan Division Code CSA Code CBSA Title Metropolitan/Micropolitan Statistical Area Metropolitan Division Title CSA Title County/County Equivalent State Name FIPS State Code FIPS County Code Central/Outlying County fips . 8 10380.0 | NaN | NaN | Aguadilla-Isabela, PR | Metropolitan Statistical Area | NaN | NaN | Aguada Municipio | Puerto Rico | 72 | 003 | Central | 72003.0 | . 9 10380.0 | NaN | NaN | Aguadilla-Isabela, PR | Metropolitan Statistical Area | NaN | NaN | Aguadilla Municipio | Puerto Rico | 72 | 005 | Central | 72005.0 | . 10 10380.0 | NaN | NaN | Aguadilla-Isabela, PR | Metropolitan Statistical Area | NaN | NaN | Añasco Municipio | Puerto Rico | 72 | 011 | Central | 72011.0 | . 11 10380.0 | NaN | NaN | Aguadilla-Isabela, PR | Metropolitan Statistical Area | NaN | NaN | Isabela Municipio | Puerto Rico | 72 | 071 | Central | 72071.0 | . 12 10380.0 | NaN | NaN | Aguadilla-Isabela, PR | Metropolitan Statistical Area | NaN | NaN | Lares Municipio | Puerto Rico | 72 | 081 | Central | 72081.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 1591 42180.0 | NaN | 434.0 | Santa Isabel, PR | Micropolitan Statistical Area | NaN | Ponce-Yauco-Coamo, PR | Santa Isabel Municipio | Puerto Rico | 72 | 133 | Central | 72133.0 | . 1903 49500.0 | NaN | 434.0 | Yauco, PR | Metropolitan Statistical Area | NaN | Ponce-Yauco-Coamo, PR | Guánica Municipio | Puerto Rico | 72 | 055 | Central | 72055.0 | . 1904 49500.0 | NaN | 434.0 | Yauco, PR | Metropolitan Statistical Area | NaN | Ponce-Yauco-Coamo, PR | Guayanilla Municipio | Puerto Rico | 72 | 059 | Central | 72059.0 | . 1905 49500.0 | NaN | 434.0 | Yauco, PR | Metropolitan Statistical Area | NaN | Ponce-Yauco-Coamo, PR | Peñuelas Municipio | Puerto Rico | 72 | 111 | Central | 72111.0 | . 1906 49500.0 | NaN | 434.0 | Yauco, PR | Metropolitan Statistical Area | NaN | Ponce-Yauco-Coamo, PR | Yauco Municipio | Puerto Rico | 72 | 153 | Central | 72153.0 | . 88 rows × 13 columns . # Puerto Rico does not provide data at county level. So we will have to do a similar exercise like NYC for PR and aggregate it statewise. # But since in albersUsa projection PR is filtered anyways, we won&#39;t be doing that exercise right away. Once I figure out how to use custom projection in the default # albersUsa projection that is used by Vega-Lite under the hood, we will include Puerto Rico aggregate[aggregate[&#39;state&#39;].str.startswith(&#39;Puerto&#39;)] . fips county date state cases deaths . 1 2.0 | Unknown | 2020-07-04 | Puerto Rico | 7787 | 155 | . aggregate[aggregate[&#39;fips&#39;].isin(msa[&#39;fips&#39;]) == False] . fips county date state cases deaths . 1 2.0 | Unknown | 2020-07-04 | Puerto Rico | 7787 | 155 | . 7 1011.0 | Bullock | 2020-07-04 | Alabama | 373 | 11 | . 8 1013.0 | Butler | 2020-07-04 | Alabama | 626 | 28 | . 11 1019.0 | Cherokee | 2020-07-04 | Alabama | 88 | 7 | . 13 1023.0 | Choctaw | 2020-07-04 | Alabama | 196 | 12 | . ... ... | ... | ... | ... | ... | ... | . 3052 56027.0 | Niobrara | 2020-07-04 | Wyoming | 2 | 0 | . 3053 56029.0 | Park | 2020-07-04 | Wyoming | 61 | 0 | . 3054 56031.0 | Platte | 2020-07-04 | Wyoming | 4 | 0 | . 3056 56035.0 | Sublette | 2020-07-04 | Wyoming | 6 | 0 | . 3060 56043.0 | Washakie | 2020-07-04 | Wyoming | 38 | 5 | . 1233 rows × 6 columns . Now we will merge the aggregated data with msa since msa is expanded on fips(CBSA repeats), which is the only unique column. . msa = msa.merge(aggregate, how=&#39;inner&#39;, on=&#39;fips&#39;) msa . CBSA Code Metropolitan Division Code CSA Code CBSA Title Metropolitan/Micropolitan Statistical Area Metropolitan Division Title CSA Title County/County Equivalent State Name FIPS State Code FIPS County Code Central/Outlying County fips county date state cases deaths . 0 10100.0 | NaN | NaN | Aberdeen, SD | Micropolitan Statistical Area | NaN | NaN | Brown County | South Dakota | 46 | 013 | Central | 46013.0 | Brown | 2020-07-04 | South Dakota | 343 | 2 | . 1 10100.0 | NaN | NaN | Aberdeen, SD | Micropolitan Statistical Area | NaN | NaN | Edmunds County | South Dakota | 46 | 045 | Outlying | 46045.0 | Edmunds | 2020-07-04 | South Dakota | 8 | 0 | . 2 10140.0 | NaN | NaN | Aberdeen, WA | Micropolitan Statistical Area | NaN | NaN | Grays Harbor County | Washington | 53 | 027 | Central | 53027.0 | Grays Harbor | 2020-07-04 | Washington | 26 | 0 | . 3 10180.0 | NaN | NaN | Abilene, TX | Metropolitan Statistical Area | NaN | NaN | Callahan County | Texas | 48 | 059 | Outlying | 48059.0 | Callahan | 2020-07-04 | Texas | 19 | 2 | . 4 10180.0 | NaN | NaN | Abilene, TX | Metropolitan Statistical Area | NaN | NaN | Jones County | Texas | 48 | 253 | Outlying | 48253.0 | Jones | 2020-07-04 | Texas | 610 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 1824 49700.0 | NaN | 472.0 | Yuba City, CA | Metropolitan Statistical Area | NaN | Sacramento-Roseville, CA | Yuba County | California | 06 | 115 | Central | 6115.0 | Yuba | 2020-07-04 | California | 120 | 2 | . 1825 49740.0 | NaN | NaN | Yuma, AZ | Metropolitan Statistical Area | NaN | NaN | Yuma County | Arizona | 04 | 027 | Central | 4027.0 | Yuma | 2020-07-04 | Arizona | 7062 | 110 | . 1826 49780.0 | NaN | 198.0 | Zanesville, OH | Micropolitan Statistical Area | NaN | Columbus-Marion-Zanesville, OH | Muskingum County | Ohio | 39 | 119 | Central | 39119.0 | Muskingum | 2020-07-04 | Ohio | 81 | 1 | . 1827 49820.0 | NaN | NaN | Zapata, TX | Micropolitan Statistical Area | NaN | NaN | Zapata County | Texas | 48 | 505 | Central | 48505.0 | Zapata | 2020-07-04 | Texas | 56 | 0 | . 1828 35620.0 | None | 408.0 | None | None | None | None | None | New York | 36 | None | None | 1.0 | New York City | 2020-07-04 | New York | 221395 | 22630 | . 1829 rows × 18 columns . msa.rename(columns={&#39;CBSA Code&#39;: &#39;CBSAFP&#39;}, inplace=True) . msa_shp[&#39;CBSAFP&#39;] = msa_shp[&#39;CBSAFP&#39;].astype(float) . msa_shp[&#39;lon&#39;] = msa_shp[&#39;geometry&#39;].centroid.x msa_shp[&#39;lat&#39;] = msa_shp[&#39;geometry&#39;].centroid.y . Now we will aggregate the msa data on CBSAFP so that it becomes similar to msa_shp geodataframe . msa_agg = msa.groupby(&#39;CBSAFP&#39;, as_index=False).agg({&#39;CBSA Title&#39;: &#39;first&#39;, &#39;State Name&#39;: &#39;last&#39;, &#39;date&#39;: &#39;last&#39;, &#39;cases&#39;: &#39;sum&#39;, &#39;deaths&#39;: &#39;sum&#39;}) . msa_agg.head() . CBSAFP CBSA Title State Name date cases deaths . 0 10100.0 | Aberdeen, SD | South Dakota | 2020-07-04 | 351 | 2 | . 1 10140.0 | Aberdeen, WA | Washington | 2020-07-04 | 26 | 0 | . 2 10180.0 | Abilene, TX | Texas | 2020-07-04 | 1209 | 8 | . 3 10220.0 | Ada, OK | Oklahoma | 2020-07-04 | 44 | 2 | . 4 10300.0 | Adrian, MI | Michigan | 2020-07-04 | 966 | 10 | . Now we will merge msa_agg with msa and make the heights column that contains a custom SVG Path string per row, since right now Vega-Lite does not support &quot;scaleY&quot; as an encoding. In Vega, we don&#39;t have to do it this way, we can just provide a single svg path for an equilateral triangle and then stretch it(using scaleY) based on &quot;cases&quot; or &quot;deaths&quot;. . msa_shp = msa_shp.merge(msa_agg, how=&#39;inner&#39;, on=&#39;CBSAFP&#39;) . msa_shp[&#39;height&#39;] = msa_shp[&#39;deaths&#39;].apply(lambda x: f&quot;M -1.5 0 L0 -{x/50} L1.5 0&quot; if pd.notnull(x) else &quot;M -1.5 0 L0 0 L1.5 0&quot;) msa_shp.head() . CSAFP CBSAFP AFFGEOID GEOID NAME LSAD ALAND AWATER geometry lon lat CBSA Title State Name date cases deaths height . 0 425 | 37620.0 | 310M500US37620 | 37620 | Parkersburg-Vienna, WV | M1 | 1551452495 | 32408833 | POLYGON ((-81.75582 39.18052, -81.75575 39.180... | -81.462789 | 39.138851 | Parkersburg-Vienna, WV | West Virginia | 2020-07-04 | 123 | 2 | M -1.5 0 L0 -0.04 L1.5 0 | . 1 None | 45980.0 | 310M500US45980 | 45980 | Troy, AL | M2 | 1740647520 | 2336975 | POLYGON ((-86.19941 31.80786, -86.19808 31.808... | -85.940915 | 31.802723 | Troy, AL | Alabama | 2020-07-04 | 427 | 5 | M -1.5 0 L0 -0.1 L1.5 0 | . 2 548 | 49020.0 | 310M500US49020 | 49020 | Winchester, VA-WV | M1 | 2752545068 | 16892497 | POLYGON ((-78.97849 39.23900, -78.97626 39.243... | -78.473885 | 39.272178 | Winchester, VA-WV | West Virginia | 2020-07-04 | 892 | 10 | M -1.5 0 L0 -0.2 L1.5 0 | . 3 142 | 45180.0 | 310M500US45180 | 45180 | Talladega-Sylacauga, AL | M2 | 1908293036 | 60927931 | POLYGON ((-86.50359 33.17598, -86.50313 33.179... | -86.165882 | 33.380087 | Talladega-Sylacauga, AL | Alabama | 2020-07-04 | 311 | 8 | M -1.5 0 L0 -0.16 L1.5 0 | . 4 None | 25060.0 | 310M500US25060 | 25060 | Gulfport-Biloxi, MS | M1 | 5739122781 | 2105780374 | MULTIPOLYGON (((-88.50297 30.21523, -88.49176 ... | -89.037857 | 30.556428 | Gulfport-Biloxi, MS | Mississippi | 2020-07-04 | 1692 | 40 | M -1.5 0 L0 -0.8 L1.5 0 | . msa_shp.drop([&#39;CSAFP&#39;, &#39;AFFGEOID&#39;, &#39;GEOID&#39;, &#39;LSAD&#39;, &#39;ALAND&#39;, &#39;AWATER&#39;], axis=1, inplace=True) . msa_shp.head() . CBSAFP NAME geometry lon lat CBSA Title State Name date cases deaths height . 0 37620.0 | Parkersburg-Vienna, WV | POLYGON ((-81.75582 39.18052, -81.75575 39.180... | -81.462789 | 39.138851 | Parkersburg-Vienna, WV | West Virginia | 2020-07-04 | 123 | 2 | M -1.5 0 L0 -0.04 L1.5 0 | . 1 45980.0 | Troy, AL | POLYGON ((-86.19941 31.80786, -86.19808 31.808... | -85.940915 | 31.802723 | Troy, AL | Alabama | 2020-07-04 | 427 | 5 | M -1.5 0 L0 -0.1 L1.5 0 | . 2 49020.0 | Winchester, VA-WV | POLYGON ((-78.97849 39.23900, -78.97626 39.243... | -78.473885 | 39.272178 | Winchester, VA-WV | West Virginia | 2020-07-04 | 892 | 10 | M -1.5 0 L0 -0.2 L1.5 0 | . 3 45180.0 | Talladega-Sylacauga, AL | POLYGON ((-86.50359 33.17598, -86.50313 33.179... | -86.165882 | 33.380087 | Talladega-Sylacauga, AL | Alabama | 2020-07-04 | 311 | 8 | M -1.5 0 L0 -0.16 L1.5 0 | . 4 25060.0 | Gulfport-Biloxi, MS | MULTIPOLYGON (((-88.50297 30.21523, -88.49176 ... | -89.037857 | 30.556428 | Gulfport-Biloxi, MS | Mississippi | 2020-07-04 | 1692 | 40 | M -1.5 0 L0 -0.8 L1.5 0 | . Let&#39;s finally plot this graph of deaths till now - . spikes = alt.Chart(msa_shp).transform_filter(alt.datum.deaths&gt;0).mark_point( fillOpacity=1, fill=alt.Gradient( gradient=&quot;linear&quot;, stops=[alt.GradientStop(color=&#39;white&#39;, offset=0), alt.GradientStop(color=&#39;red&#39;, offset=0.5)], x1=1, x2=1, y1=1, y2=0 ), #dx=10, #dy=-30, strokeOpacity=1, strokeWidth=1, stroke=&#39;red&#39; ).encode( latitude=&quot;lat:Q&quot;, longitude=&quot;lon:Q&quot;, shape=alt.Shape(&quot;height:N&quot;, scale=None), #tooltip=[&#39;CBSA Title:N&#39;, &#39;deaths:Q&#39;], #color = alt.condition(selection, alt.value(&#39;black&#39;), alt.value(&#39;red&#39;)) ).project( type=&#39;albersUsa&#39; ).properties( width=1200, height=800 ) state = alt.Chart(states).mark_geoshape(fill=&#39;#ededed&#39;, stroke=&#39;white&#39;).encode( ).project( type=&#39;albersUsa&#39; ) (state+spikes).configure_view(strokeWidth=0) . Now we will study only last week&#39;s average cases per day to see where the indections are on the rise . #msa_shp[&#39;height_cases&#39;] = msa_shp[&#39;cases&#39;].apply(lambda x: f&quot;M -1.5 0 L0 -{x/1000} L1.5 0&quot; if pd.notnull(x) else &quot;M -1.5 0 L0 0 L1.5 0&quot;) . cdf[&#39;cases_per_day&#39;] = cdf.groupby(&quot;fips&quot;)[&#39;cases&#39;].diff() . last_week_cases_avg = cdf.groupby(&quot;fips&quot;)[&quot;cases_per_day&quot;].apply(lambda x: x.iloc[-7:].mean()) . last_week_cases_avg = last_week_cases_avg.reset_index() . last_week_cases_avg.columns = [&#39;fips&#39;, &#39;avg_cases_last_week&#39;] . last_week_cases_avg.head() . fips avg_cases_last_week . 0 1.0 | 319.714286 | . 1 2.0 | 103.000000 | . 2 1001.0 | 13.285714 | . 3 1003.0 | 44.000000 | . 4 1005.0 | 4.714286 | . avg_last_week_cases = cdf.groupby(&quot;fips&quot;).agg({&#39;county&#39;: &#39;first&#39;, &#39;cases_per_day&#39;: &#39;last&#39;, &#39;date&#39;: &#39;last&#39;, &#39;state&#39;: &#39;last&#39;, &#39;cases&#39;: &#39;last&#39;, &#39;deaths&#39;: &#39;last&#39;}) . avg_last_week_cases = avg_last_week_cases.merge(last_week_cases_avg, how=&#39;inner&#39;, on=&#39;fips&#39;) . avg_last_week_cases.head() . fips county cases_per_day date state cases deaths avg_cases_last_week . 0 1.0 | New York City | 367.0 | 2020-07-04 | New York | 221395 | 22630 | 319.714286 | . 1 2.0 | Unknown | 104.0 | 2020-07-04 | Puerto Rico | 7787 | 155 | 103.000000 | . 2 1001.0 | Autauga | 23.0 | 2020-07-04 | Alabama | 591 | 13 | 13.285714 | . 3 1003.0 | Baldwin | 18.0 | 2020-07-04 | Alabama | 863 | 10 | 44.000000 | . 4 1005.0 | Barbour | 2.0 | 2020-07-04 | Alabama | 350 | 2 | 4.714286 | . msa_agg_lastweek = msa.merge(avg_last_week_cases, how=&#39;inner&#39;, on=&#39;fips&#39;) msa_agg_lastweek.head() . CBSAFP Metropolitan Division Code CSA Code CBSA Title Metropolitan/Micropolitan Statistical Area Metropolitan Division Title CSA Title County/County Equivalent State Name FIPS State Code ... state_x cases_x deaths_x county_y cases_per_day date_y state_y cases_y deaths_y avg_cases_last_week . 0 10100.0 | NaN | NaN | Aberdeen, SD | Micropolitan Statistical Area | NaN | NaN | Brown County | South Dakota | 46 | ... | South Dakota | 343 | 2 | Brown | -1.0 | 2020-07-04 | South Dakota | 343 | 2 | 0.571429 | . 1 10100.0 | NaN | NaN | Aberdeen, SD | Micropolitan Statistical Area | NaN | NaN | Edmunds County | South Dakota | 46 | ... | South Dakota | 8 | 0 | Edmunds | 1.0 | 2020-07-04 | South Dakota | 8 | 0 | 0.142857 | . 2 10140.0 | NaN | NaN | Aberdeen, WA | Micropolitan Statistical Area | NaN | NaN | Grays Harbor County | Washington | 53 | ... | Washington | 26 | 0 | Grays Harbor | 0.0 | 2020-07-04 | Washington | 26 | 0 | 0.142857 | . 3 10180.0 | NaN | NaN | Abilene, TX | Metropolitan Statistical Area | NaN | NaN | Callahan County | Texas | 48 | ... | Texas | 19 | 2 | Callahan | 1.0 | 2020-07-04 | Texas | 19 | 2 | 0.428571 | . 4 10180.0 | NaN | NaN | Abilene, TX | Metropolitan Statistical Area | NaN | NaN | Jones County | Texas | 48 | ... | Texas | 610 | 0 | Jones | 0.0 | 2020-07-04 | Texas | 610 | 0 | 0.428571 | . 5 rows × 25 columns . # Average of multiple series is same as sum of their averages so &#39;avg_cases_last_week&#39;: &#39;sum&#39; works well msa_agg_lastweek = msa_agg_lastweek.groupby(&#39;CBSAFP&#39;, as_index=False).agg({&#39;CBSA Title&#39;: &#39;first&#39;, &#39;State Name&#39;: &#39;last&#39;, &#39;avg_cases_last_week&#39;: &#39;sum&#39;}) . msa_agg_lastweek.head() . CBSAFP CBSA Title State Name avg_cases_last_week . 0 10100.0 | Aberdeen, SD | South Dakota | 0.714286 | . 1 10140.0 | Aberdeen, WA | Washington | 0.142857 | . 2 10180.0 | Abilene, TX | Texas | 21.857143 | . 3 10220.0 | Ada, OK | Oklahoma | 1.285714 | . 4 10300.0 | Adrian, MI | Michigan | 4.000000 | . msa_shp = msa_shp.merge(msa_agg_lastweek, how=&#39;inner&#39;, on=&#39;CBSAFP&#39;) msa_shp.head() . CBSAFP NAME geometry lon lat CBSA Title_x State Name_x date cases deaths height CBSA Title_y State Name_y avg_cases_last_week . 0 37620.0 | Parkersburg-Vienna, WV | POLYGON ((-81.75582 39.18052, -81.75575 39.180... | -81.462789 | 39.138851 | Parkersburg-Vienna, WV | West Virginia | 2020-07-04 | 123 | 2 | M -1.5 0 L0 -0.04 L1.5 0 | Parkersburg-Vienna, WV | West Virginia | 6.428571 | . 1 45980.0 | Troy, AL | POLYGON ((-86.19941 31.80786, -86.19808 31.808... | -85.940915 | 31.802723 | Troy, AL | Alabama | 2020-07-04 | 427 | 5 | M -1.5 0 L0 -0.1 L1.5 0 | Troy, AL | Alabama | 4.714286 | . 2 49020.0 | Winchester, VA-WV | POLYGON ((-78.97849 39.23900, -78.97626 39.243... | -78.473885 | 39.272178 | Winchester, VA-WV | West Virginia | 2020-07-04 | 892 | 10 | M -1.5 0 L0 -0.2 L1.5 0 | Winchester, VA-WV | West Virginia | 9.000000 | . 3 45180.0 | Talladega-Sylacauga, AL | POLYGON ((-86.50359 33.17598, -86.50313 33.179... | -86.165882 | 33.380087 | Talladega-Sylacauga, AL | Alabama | 2020-07-04 | 311 | 8 | M -1.5 0 L0 -0.16 L1.5 0 | Talladega-Sylacauga, AL | Alabama | 11.857143 | . 4 25060.0 | Gulfport-Biloxi, MS | MULTIPOLYGON (((-88.50297 30.21523, -88.49176 ... | -89.037857 | 30.556428 | Gulfport-Biloxi, MS | Mississippi | 2020-07-04 | 1692 | 40 | M -1.5 0 L0 -0.8 L1.5 0 | Gulfport-Biloxi, MS | Mississippi | 60.000000 | . Adding a new column called height_last_week_avg_cases that has the custom SVG paths like we did earlier - . msa_shp[&#39;height_last_week_avg_cases&#39;] = msa_shp[&#39;avg_cases_last_week&#39;].apply(lambda x: f&quot;M -1.5 0 L0 -{x/10} L1.5 0&quot; if pd.notnull(x) else &quot;M -1.5 0 L0 0 L1.5 0&quot;) . spikes = alt.Chart(msa_shp).mark_point( fillOpacity=1, fill=alt.Gradient( gradient=&quot;linear&quot;, stops=[alt.GradientStop(color=&#39;white&#39;, offset=0), alt.GradientStop(color=&#39;red&#39;, offset=0.5)], x1=1, x2=1, y1=1, y2=0 ), #dx=10, #dy=-30, stroke=&#39;red&#39;, strokeOpacity=1, strokeWidth=1 ).encode( latitude=&quot;lat:Q&quot;, longitude=&quot;lon:Q&quot;, shape=alt.Shape(&quot;height_last_week_avg_cases:N&quot;, scale=None), tooltip=[&#39;NAME:N&#39;, &#39;avg_cases_last_week:Q&#39;] ).project( type=&#39;albersUsa&#39; ).properties( width=1200, height=800 ) state = alt.Chart(states).mark_geoshape(fill=&#39;#ededed&#39;, stroke=&#39;white&#39;).encode( ).project( type=&#39;albersUsa&#39; ) (state+spikes).configure_view(strokeWidth=0) . It&#39;s clear that now the cases are increasing much rapidly in the South especially in the states of - Texas, California, Florida and Arizona . There you have it! .",
            "url": "https://armsp.github.io/covidviz/geospatial/nyt/2020/07/01/MSA-cases-and-deaths.html",
            "relUrl": "/geospatial/nyt/2020/07/01/MSA-cases-and-deaths.html",
            "date": " • Jul 1, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Per Capita COVID-19 cases worldwide",
            "content": "Today we will make the Per Capita covid cases worldwide from the article Coronavirus Map: Tracking the Global Outbreak that looks like the following - . . import pandas as pd import geopandas as gpd import altair as alt . We will use the JHU CSSE Dataset for the cases as well as the population. For the map we will use a geojson file that I made in the last world chloropleth case growth map. . population_uri = &#39;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/UID_ISO_FIPS_LookUp_Table.csv&#39; population_data = pd.read_csv(population_uri) latest_cases_uri = &#39;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/06-24-2020.csv&#39; latest_cases = pd.read_csv(latest_cases_uri) world_geojson = &#39;https://raw.githubusercontent.com/armsp/covidviz/master/assets/world.geojson&#39; world = gpd.read_file(world_geojson) . Dropping unnecessary columns and making necessary moniker changes to countries just like we did last time - . latest_cases = latest_cases.drop([&#39;FIPS&#39;, &#39;Admin2&#39;, &#39;Province_State&#39;, &#39;Last_Update&#39;, &#39;Lat&#39;, &#39;Long_&#39;, &#39;Combined_Key&#39;, &#39;Incidence_Rate&#39;, &#39;Case-Fatality_Ratio&#39;], axis=1) latest_cases = latest_cases.groupby(&#39;Country_Region&#39;).aggregate({&#39;Confirmed&#39;: &#39;sum&#39;, &#39;Recovered&#39;: &#39;sum&#39;, &#39;Deaths&#39;: &#39;sum&#39;, &#39;Active&#39;: &#39;sum&#39;, }) latest_cases = latest_cases.reset_index() . latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;Taiwan*&#39;, &#39;Country_Region&#39;] = &#39;Taiwan&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;US&#39;, &#39;Country_Region&#39;] = &#39;United States&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;Czech Republic&#39;, &#39;Country_Region&#39;] = &#39;Czechia&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;West Bank and Gaza&#39;, &#39;Country_Region&#39;] = &#39;West Bank (disp)&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;Western Sahara&#39;, &#39;Country_Region&#39;] = &#39;Western Sahara (disp)&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;Trinidad and Tobago&#39;, &#39;Country_Region&#39;] = &#39;Trinidad &amp; Tobago&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;Sao Tome and Principe&#39;, &#39;Country_Region&#39;] = &#39;Sao Tome &amp; Principe&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;Saint Vincent and the Grenadines&#39;, &#39;Country_Region&#39;] = &#39;St Vincent &amp; the Grenadines&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;Saint Lucia&#39;, &#39;Country_Region&#39;] = &#39;St Lucia&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;Saint Kitts and Nevis&#39;, &#39;Country_Region&#39;] = &#39;St Kitts &amp; Nevis&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;North Macedonia&#39;, &#39;Country_Region&#39;] = &#39;Macedonia&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;Bahamas&#39;, &#39;Country_Region&#39;] = &#39;Bahamas, The&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;Bosnia and Herzegovina&#39;, &#39;Country_Region&#39;] = &#39;Bosnia &amp; Herzegovina&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;Central African Republic&#39;, &#39;Country_Region&#39;] = &#39;Central African Rep&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;Eswatini&#39;, &#39;Country_Region&#39;] = &#39;Swaziland&#39; #time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;South Korea&#39;, &#39;Country/Region&#39;] = &#39;Korea, South&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;Congo (Kinshasa)&#39;, &#39;Country_Region&#39;] = &#39;Congo, Dem Rep of the&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;Congo (Brazzaville)&#39;, &#39;Country_Region&#39;] = &#39;Congo, Rep of the&#39; latest_cases.loc[latest_cases[&#39;Country_Region&#39;]==&#39;Antigua and Barbuda&#39;, &#39;Country_Region&#39;] = &#39;Antigua &amp; Barbuda&#39; . latest_cases[latest_cases[&#39;Country_Region&#39;].isin(world[&#39;COUNTRY_NA&#39;]) == False] . Country_Region Confirmed Recovered Deaths Active . 48 Diamond Princess | 712 | 651 | 13 | 48 | . 64 Gambia | 42 | 26 | 2 | 14 | . 75 Holy See | 12 | 12 | 0 | 0 | . 104 MS Zaandam | 9 | 0 | 2 | 7 | . Extracting population data for countries - . population_data = population_data.drop([&#39;UID&#39;, &#39;code3&#39;, &#39;FIPS&#39;, &#39;Admin2&#39;, &#39;Province_State&#39;, &#39;Lat&#39;, &#39;Long_&#39;], axis=1) population_data = population_data[population_data[&#39;Country_Region&#39;] == population_data[&#39;Combined_Key&#39;]] population_data = population_data.reset_index(drop=True) population_data.head() . iso2 iso3 Country_Region Combined_Key Population . 0 AF | AFG | Afghanistan | Afghanistan | 38928341.0 | . 1 AL | ALB | Albania | Albania | 2877800.0 | . 2 DZ | DZA | Algeria | Algeria | 43851043.0 | . 3 AD | AND | Andorra | Andorra | 77265.0 | . 4 AO | AGO | Angola | Angola | 32866268.0 | . population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;Taiwan*&#39;, &#39;Country_Region&#39;] = &#39;Taiwan&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;US&#39;, &#39;Country_Region&#39;] = &#39;United States&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;Czech Republic&#39;, &#39;Country_Region&#39;] = &#39;Czechia&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;West Bank and Gaza&#39;, &#39;Country_Region&#39;] = &#39;West Bank (disp)&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;Western Sahara&#39;, &#39;Country_Region&#39;] = &#39;Western Sahara (disp)&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;Trinidad and Tobago&#39;, &#39;Country_Region&#39;] = &#39;Trinidad &amp; Tobago&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;Sao Tome and Principe&#39;, &#39;Country_Region&#39;] = &#39;Sao Tome &amp; Principe&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;Saint Vincent and the Grenadines&#39;, &#39;Country_Region&#39;] = &#39;St Vincent &amp; the Grenadines&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;Saint Lucia&#39;, &#39;Country_Region&#39;] = &#39;St Lucia&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;Saint Kitts and Nevis&#39;, &#39;Country_Region&#39;] = &#39;St Kitts &amp; Nevis&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;North Macedonia&#39;, &#39;Country_Region&#39;] = &#39;Macedonia&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;Bahamas&#39;, &#39;Country_Region&#39;] = &#39;Bahamas, The&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;Bosnia and Herzegovina&#39;, &#39;Country_Region&#39;] = &#39;Bosnia &amp; Herzegovina&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;Central African Republic&#39;, &#39;Country_Region&#39;] = &#39;Central African Rep&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;Eswatini&#39;, &#39;Country_Region&#39;] = &#39;Swaziland&#39; #time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;South Korea&#39;, &#39;Country/Region&#39;] = &#39;Korea, South&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;Congo (Kinshasa)&#39;, &#39;Country_Region&#39;] = &#39;Congo, Dem Rep of the&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;Congo (Brazzaville)&#39;, &#39;Country_Region&#39;] = &#39;Congo, Rep of the&#39; population_data.loc[population_data[&#39;Country_Region&#39;]==&#39;Antigua and Barbuda&#39;, &#39;Country_Region&#39;] = &#39;Antigua &amp; Barbuda&#39; . world.columns = [&#39;Country_Region&#39;, &#39;geometry&#39;] world = world.merge(latest_cases, on=&#39;Country_Region&#39;, how=&#39;left&#39;) world = world.merge(population_data, on=&#39;Country_Region&#39;, how=&#39;left&#39;) world[&#39;per_capita&#39;] = world[&#39;Confirmed&#39;]/world[&#39;Population&#39;] . world[&#39;code&#39;] = world[&#39;per_capita&#39;].apply(lambda x: &#39;Less than 1 in 1000&#39; if x &lt;= (1/1000) else &#39;Less than 1 in 500&#39; if x&lt;= (1/500) else &#39;Less than 1 in 333&#39; if x&lt;= (1/333) else &#39;No Cases reported&#39; if pd.isnull(x) else &#39;Greater than 1 in 333&#39;) world[&#39;Share of Population&#39;] = world[&#39;Population&#39;]/world[&#39;Confirmed&#39;] world[&#39;Share of Population&#39;] = world[&#39;Share of Population&#39;].round() world[&#39;Share of Population&#39;] = world[&#39;Share of Population&#39;].apply(lambda x: f&quot;1 in {str(x).split(&#39;.&#39;)[0]}&quot;) . Making the chart - . alt.Chart(world).mark_geoshape(stroke=&#39;white&#39;).transform_filter(alt.datum.Country_Region != &#39;Antarctica&#39;).encode( color=alt.Color(&#39;code:N&#39;, scale=alt.Scale(domain=[&#39;No Cases reported&#39;, &#39;Less than 1 in 1000&#39;, &#39;Less than 1 in 500&#39;, &#39;Less than 1 in 333&#39;, &#39;Greater than 1 in 333&#39;], range=[&#39;lightgrey&#39;, &#39;#f2df91&#39;, &#39;#ffae43&#39;, &#39;#ff6e0b&#39;, &#39;#ce0a05&#39;]),legend=alt.Legend(title=None, orient=&#39;top&#39;, labelBaseline=&#39;middle&#39;, symbolType=&#39;square&#39;, columnPadding=20, labelFontSize=15, gridAlign=&#39;each&#39;, symbolSize=200)), tooltip = [&#39;Country_Region&#39;, &#39;Confirmed&#39;, &#39;Share of Population&#39;] ).properties(width=1400, height=800).project(&#39;equalEarth&#39;).configure_view(strokeWidth=0) .",
            "url": "https://armsp.github.io/covidviz/geospatial/interactive/chloropleth/nyt/2020/06/28/World-Per-Capita-Cases.html",
            "relUrl": "/geospatial/interactive/chloropleth/nyt/2020/06/28/World-Per-Capita-Cases.html",
            "date": " • Jun 28, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Temporary decline in CO₂ due to COVID-19",
            "content": "Today we will work on the following graph from the article Emissions Are Surging Back as Countries and States Reopen - . . I downloaded the dataset as an Excel file and saved data for individual countries as csv files. . import altair as alt import pandas as pd from functools import wraps import datetime as dt . #hide_output alt.renderers.set_embed_options(actions=False) def log_step(func): @wraps(func) def wrapper(*args, **kwargs): &quot;&quot;&quot;timing or logging etc&quot;&quot;&quot; start = dt.datetime.now() output = func(*args, **kwargs) end = dt.datetime.now() print(f&quot;After function {func.__name__} ran, shape of dataframe is - {output.shape}, execution time is - {end-start}&quot;) return output return wrapper @log_step def read_concat_country_data(): india = pd.read_csv(&#39;ind_co2_em.csv&#39;) india = india.iloc[1:] china = pd.read_csv(&#39;china_co2_em.csv&#39;, sep=&#39;;&#39;) china = china.iloc[1:] us = pd.read_csv(&#39;us_co2_em.csv&#39;, sep=&#39;;&#39;) us = us.iloc[1:] euuk = pd.read_csv(&#39;euuk_co2_em.csv&#39;, sep=&#39;;&#39;) euuk = euuk.iloc[1:] globl = pd.read_csv(&#39;global_co2_em.csv&#39;, sep=&#39;;&#39;) globl = globl.iloc[1:] data = pd.concat([china, india, euuk, us, globl]) return data @log_step def drop_columns(df, cols): df.drop(columns = cols, inplace=True) return df def set_datatypes(df): df[&#39;DATE&#39;] = pd.to_datetime(df[&#39;DATE&#39;],format=&#39;%d/%m/%Y&#39;) df[list(df.columns)[3:]] = df[list(df.columns)[3:]].apply(pd.to_numeric) return df @log_step def make_plotting_data(df): &#39;&#39;&#39;Remove GLOBAL, subtract the sum of countries emissions from GLOBAL to get REST (of the world) data&#39;&#39;&#39; except_global_data = df[df[&#39;REGION_CODE&#39;] != &#39;GLOBAL&#39;] global_data = df[df[&#39;REGION_CODE&#39;] == &#39;GLOBAL&#39;].reset_index(drop=True) countries_emissions = except_global_data.groupby(&#39;DATE&#39;, as_index=False).sum()#.reindex(except_global_data.columns, axis=1).fillna({&#39;REGION_CODE&#39;: &#39;RST&#39;, &#39;REGION_NAME&#39;: &#39;REST&#39;}) rest_emissions_data = global_data[list(global_data.columns)[3:]] - countries_emissions#[list(countries_emissions.columns)[5:]] rest_emissions_data = rest_emissions_data.reindex(global_data.columns, axis=1).fillna({&#39;REGION_CODE&#39;: &#39;RST&#39;, &#39;REGION_NAME&#39;: &#39;REST&#39;, &#39;DATE&#39;: global_data[&#39;DATE&#39;]}) plot_data = pd.concat([except_global_data, rest_emissions_data]) return plot_data . emission_data = (read_concat_country_data() .pipe(drop_columns, *[[&#39;REGION_ID&#39;, &#39;TIME_POINT&#39;]]) .pipe(set_datatypes)) emission_data.head() . After function read_concat_country_data ran, shape of dataframe is - (815, 26), execution time is - 0:00:00.055744 After function drop_columns ran, shape of dataframe is - (815, 24), execution time is - 0:00:00.001328 . REGION_CODE REGION_NAME DATE TOTAL_CO2_MED PWR_CO2_MED IND_CO2_MED TRS_CO2_MED PUB_CO2_MED RES_CO2_MED AVI_CO2_MED ... PUB_CO2_LOW RES_CO2_LOW AVI_CO2_LOW TOTAL_CO2_HIGH PWR_CO2_HIGH IND_CO2_HIGH TRS_CO2_HIGH PUB_CO2_HIGH RES_CO2_HIGH AVI_CO2_HIGH . 1 CHN | China | 2020-01-01 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2 CHN | China | 2020-01-02 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 3 CHN | China | 2020-01-03 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 4 CHN | China | 2020-01-04 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 5 CHN | China | 2020-01-05 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 5 rows × 24 columns . If you observe the chart closely you will realize that the graph is stacked, so that is what we will do right away using altair&#39;s area chart - . alt.Chart(emission_data).mark_area().encode( x=alt.X(&#39;DATE:T&#39;), y=alt.Y(&#39;TOTAL_CO2_MED:Q&#39;), color=alt.Color(&#39;REGION_NAME:N&#39;),#,scale=alt.Scale(scheme=&#39;reds&#39;)), ).properties(width=800, height=400) . This is close but not exactly like what we saw in the article. If you look closely you&#39;d realize that the order of countries is different. So we will try to follow the same order using the order encoding field. . alt.Chart(emission_data).mark_area().transform_calculate(order=&quot;{&#39;CHN&#39;: 0, &#39;IND&#39;: 1, &#39;EUandUK&#39;: 2, &#39;USA&#39;: 3, &#39;GLOBAL&#39;: 4}[datum.REGION_CODE]&quot;).encode( x=alt.X(&#39;DATE:T&#39;), y=alt.Y(&#39;TOTAL_CO2_MED:Q&#39;), color=alt.Color(&#39;REGION_CODE:N&#39;),#,scale=alt.Scale(scheme=&#39;reds&#39;)), order=&#39;order:O&#39; ).properties(width=800, height=400) . #This is exactly like it. Let&#39;s change the colors, I probably would have done it the following way - # alt.Chart(emission_data).mark_area().transform_calculate(order=&quot;{&#39;CHN&#39;: 0, &#39;IND&#39;: 1, &#39;EUandUK&#39;: 2, &#39;USA&#39;: 3, &#39;GLOBAL&#39;: 4}[datum.REGION_CODE]&quot;).encode( # x=alt.X(&#39;DATE:T&#39;), # y=alt.Y(&#39;TOTAL_CO2_MED:Q&#39;), # color=alt.Color(&#39;REGION_CODE:N&#39;,scale=alt.Scale(domain=[&#39;CHN&#39;, &#39;IND&#39;, &#39;EUandUK&#39;, &#39;USA&#39;, &#39;GLOBAL&#39;], range=[&quot;#c9c9c9&quot;, &quot;#aaaaaa&quot;, &quot;#888888&quot;, &quot;#686868&quot;, &quot;#454545&quot;])), # order=&#39;order:O&#39; # ).properties(width=800, height=400) . To make it just like the graph in the article, we will get the colors from here . alt.Chart(emission_data).mark_area().transform_calculate(order=&quot;{&#39;CHN&#39;: 0, &#39;IND&#39;: 1, &#39;EUandUK&#39;: 2, &#39;USA&#39;: 3, &#39;GLOBAL&#39;: 4}[datum.REGION_CODE]&quot;).encode( x=alt.X(&#39;DATE:T&#39;), y=alt.Y(&#39;TOTAL_CO2_MED:Q&#39;), color=alt.Color(&#39;REGION_CODE:N&#39;,scale=alt.Scale(domain=[&#39;CHN&#39;, &#39;IND&#39;, &#39;EUandUK&#39;, &#39;USA&#39;, &#39;GLOBAL&#39;], range=[&quot;#fde9d1&quot;, &quot;#fcd08b&quot;, &quot;#f9b382&quot;, &quot;#e38875&quot;, &quot;#ac7066&quot;])), order=&#39;order:O&#39; ).properties(width=800, height=400) . If you look closely, you would notice that we are capturing the trend perfectly, however the area for &quot;REST of the world&quot;(GLOBAL) is much more than what it should be. That is because, its duplicating the data from US, EU, India, and China. So we need to subtract the contributions of these places from the global data and then stack them. . plot_data = emission_data.pipe(make_plotting_data) . After function make_plotting_data ran, shape of dataframe is - (815, 24), execution time is - 0:00:00.063036 . alt.Chart(plot_data).mark_area().transform_calculate(order=&quot;{&#39;CHN&#39;: 0, &#39;IND&#39;: 1, &#39;EUandUK&#39;: 2, &#39;USA&#39;: 3, &#39;RST&#39;: 4}[datum.REGION_CODE]&quot;).encode( x=alt.X(&#39;DATE:T&#39;, axis=alt.Axis(format=(&quot;%B&quot;))), y=alt.Y(&#39;TOTAL_CO2_MED:Q&#39;), color=alt.Color(&#39;REGION_CODE:N&#39;,scale=alt.Scale(domain=[&#39;CHN&#39;, &#39;IND&#39;, &#39;EUandUK&#39;, &#39;USA&#39;, &#39;RST&#39;], range=[&quot;#fde9d1&quot;, &quot;#fcd08b&quot;, &quot;#f9b382&quot;, &quot;#e38875&quot;, &quot;#ac7066&quot;])), order=&#39;order:O&#39; ).properties(width=800, height=400).configure_view(strokeWidth=0).configure_axis(grid=False) . This looks exactly like the chart in the article. Right now there is no way to properly add text in a stacked chart&#39;s corresponding area, but let&#39;s try it anyways so that once this option is available in Vega-Lite we will fix this code immediately later on. . base = alt.Chart(plot_data).mark_area().transform_calculate(order=&quot;{&#39;CHN&#39;: 0, &#39;IND&#39;: 1, &#39;EUandUK&#39;: 2, &#39;USA&#39;: 3, &#39;RST&#39;: 4}[datum.REGION_CODE]&quot;).encode( x=alt.X(&#39;DATE:T&#39;, axis=alt.Axis(format=(&quot;%B&quot;))), y=alt.Y(&#39;TOTAL_CO2_MED:Q&#39;), color=alt.Color(&#39;REGION_CODE:N&#39;,scale=alt.Scale(domain=[&#39;CHN&#39;, &#39;IND&#39;, &#39;EUandUK&#39;, &#39;USA&#39;, &#39;RST&#39;], range=[&quot;#fde9d1&quot;, &quot;#fcd08b&quot;, &quot;#f9b382&quot;, &quot;#e38875&quot;, &quot;#ac7066&quot;])), order=&#39;order:O&#39; ).properties(width=800, height=400) text = alt.Chart(plot_data).mark_text().encode( x=alt.X(&#39;DATE:T&#39;, aggregate=&#39;median&#39;, ), #y=alt.Y(&#39;variety:N&#39;), #detail=&#39;REGION_CODE:N&#39;, text=alt.Text(&#39;REGION_NAME:N&#39;), y=&#39;min(TOTAL_CO2_MED):Q&#39;, #text=&#39;REGION_NAME:N&#39; ) (base+text).configure_view(strokeWidth=0).configure_axis(grid=False) . You can get clever about it and provide hardcoded positions for text and then plot it so that&#39;s what we will do - . We will get the dates where TOTAL_CO2_MED is minimum for each region and add out hardcoded positions to it . plot_data = plot_data.reset_index(drop=True) #Important since indices repeat due to concatenation text_position = plot_data.loc[plot_data.groupby(&#39;REGION_NAME&#39;)[&#39;TOTAL_CO2_MED&#39;].idxmin(), [&#39;DATE&#39;, &#39;REGION_NAME&#39;]].reset_index(drop=True) text_position . DATE REGION_NAME . 0 2020-02-18 | China | . 1 2020-04-01 | EU and UK | . 2 2020-03-28 | India | . 3 2020-04-09 | REST | . 4 2020-04-12 | USA | . text_position[&#39;POSITION&#39;] = [-2,-4,-2,-14,-7] text_position[&#39;REGION_NAME&#39;] = [&#39;China&#39;, &#39;E.U. and Britain&#39;,&#39;India&#39;, &#39;Rest of the world&#39;, &#39;United States&#39;,] text_position . DATE REGION_NAME POSITION . 0 2020-02-18 | China | -2 | . 1 2020-04-01 | E.U. and Britain | -4 | . 2 2020-03-28 | India | -2 | . 3 2020-04-09 | Rest of the world | -14 | . 4 2020-04-12 | United States | -7 | . base = alt.Chart(plot_data).mark_area().transform_calculate(order=&quot;{&#39;CHN&#39;: 0, &#39;IND&#39;: 1, &#39;EUandUK&#39;: 2, &#39;USA&#39;: 3, &#39;RST&#39;: 4}[datum.REGION_CODE]&quot;).encode( x=alt.X(&#39;DATE:T&#39;, axis=alt.Axis(format=(&quot;%B&quot;), orient=&#39;top&#39;, tickCount=6), title=None), y=alt.Y(&#39;TOTAL_CO2_MED:Q&#39;, title=&quot;Million metric tons CO₂&quot;, axis=alt.Axis(domain=False)), color=alt.Color(&#39;REGION_CODE:N&#39;, legend=None, scale=alt.Scale(domain=[&#39;CHN&#39;, &#39;IND&#39;, &#39;EUandUK&#39;, &#39;USA&#39;, &#39;RST&#39;], range=[&quot;#fde9d1&quot;, &quot;#fcd08b&quot;, &quot;#f9b382&quot;, &quot;#e38875&quot;, &quot;#ac7066&quot;])), order=&#39;order:O&#39; ).properties(width=800, height=400) text = alt.Chart(text_position).mark_text(size=13).encode( x=alt.X(&#39;DATE:T&#39;), #y=alt.Y(&#39;variety:N&#39;), #detail=&#39;REGION_CODE:N&#39;, text=alt.Text(&#39;REGION_NAME:N&#39;), y=&#39;POSITION:Q&#39;, #text=&#39;REGION_NAME:N&#39; ) (base+text).configure_view(strokeWidth=0).configure_axis(grid=False) . While we are at it we can also make the following graph of global emissions by sector - . . The main idea behind these plots is layering an area plot on top of a line chart with the area shaded by the LOW and HIGH columns - . global_emission = pd.read_csv(&#39;global_co2_em.csv&#39;, sep=&#39;;&#39;) global_emission = global_emission.iloc[1:] global_emission = (global_emission .pipe(drop_columns, *[[&#39;REGION_ID&#39;, &#39;TIME_POINT&#39;, &#39;REGION_CODE&#39;, &#39;REGION_NAME&#39;, &#39;TOTAL_CO2_MED&#39;, &#39;TOTAL_CO2_HIGH&#39;, &#39;TOTAL_CO2_LOW&#39;]]) .pipe(set_datatypes)) . After function drop_columns ran, shape of dataframe is - (163, 19), execution time is - 0:00:00.001131 . line = alt.Chart(global_emission).mark_line().encode( x=&#39;DATE:T&#39;, y=alt.Y(&#39;TRS_CO2_MED:Q&#39;), ) band = line.mark_area(opacity=0.3).encode( x=&#39;DATE:T&#39;, y=alt.Y(&#39;TRS_CO2_LOW:Q&#39;), y2=alt.Y2(&#39;TRS_CO2_HIGH:Q&#39;), ) line+band . Now we are going to change the data so that we can facet it properly like in the article&#39;s chart - . data = pd.concat([pd.melt(global_emission.filter(regex=&#39;_MED|DATE&#39;), id_vars=[&#39;DATE&#39;], var_name=&#39;MED_KEY&#39;, value_name=&#39;MED_VALUES&#39;), pd.melt(global_emission.filter(regex=&#39;_HIGH|DATE&#39;), id_vars=[&#39;DATE&#39;], var_name=&#39;HIGH_KEY&#39;, value_name=&#39;HIGH_VALUES&#39;), pd.melt(global_emission.filter(regex=&#39;_LOW|DATE&#39;), id_vars=[&#39;DATE&#39;], var_name=&#39;LOW_KEY&#39;, value_name=&#39;LOW_VALUES&#39;)], axis=1).T.drop_duplicates().T . data = data.assign(sector = data[&#39;MED_KEY&#39;].apply(lambda x: &quot;Road transportation and shipping&quot; if x.startswith(&#39;TRS&#39;) else &quot;Industry&quot; if x.startswith(&#39;IND&#39;) else &quot;Power&quot; if x.startswith(&#39;PWR&#39;) else &quot;Aviation&quot; if x.startswith(&#39;AVI&#39;) else &quot;Public buildings and commerce&quot; if x.startswith(&#39;PUB&#39;) else &quot;Residential&quot;)) #data . area_low_high = alt.Chart().mark_area(opacity=0.5).encode( x=alt.X(&#39;DATE:T&#39;, axis=alt.Axis(format=&quot;%b&quot;)), y2= &#39;HIGH_VALUES:Q&#39;, y= alt.Y(&#39;LOW_VALUES:Q&#39;, axis=alt.Axis(domain=False, tickCount=5)) ) line_med = alt.Chart().mark_line().encode( x=&#39;DATE:T&#39;, y=&#39;MED_VALUES:Q&#39; ) alt.layer(area_low_high, line_med, data=data).facet( facet=alt.Column(&#39;sector:N&#39;, title=&quot;Change in global CO u2082 emissions by sector&quot;, sort=[&#39;Road transportation and shipping&#39;, &#39;Industry&#39;, &#39;Power&#39;, &#39;Aviation&#39;, &#39;Public buildings and commerce&#39;, &#39;Residential&#39;], header=alt.Header(labelFontSize=15, labelAnchor=&#39;start&#39;, labelFontWeight=&#39;bold&#39;) ), columns=3, ).configure_axis(grid=False, title=None).configure_axisX(orient=&#39;top&#39;, labelPadding=20, offset=-27).configure_view(strokeWidth=0).resolve_scale(x=&#39;independent&#39;).configure_header( titleFontSize=20, labelFontSize=14, titlePadding=50 ) .",
            "url": "https://armsp.github.io/covidviz/climate,/emission,/nyt/2020/06/26/CO2-Emissions.html",
            "relUrl": "/climate,/emission,/nyt/2020/06/26/CO2-Emissions.html",
            "date": " • Jun 26, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "World Geospatial chloropleth plot of cases",
            "content": "Today we will make a chloropleth of the countries in a world map like in the article Coronavirus Map: Tracking the Global Outbreak that looks like this - . For this we will use the JHU CSSE Dataset . #hide_output import pandas as pd import geopandas as gpd import altair as alt import numpy as np alt.renderers.set_embed_options(actions=False) . I made the following geojson file from the US State Department&#39;s Global LSIB Polygons Detailed after simplifying it as it has too much details and is very large. Following is the code to do that. . Warning: Do NOT RUN THE FOLLOWING CELL. USe the geojson file I have provided - run the cell following the following cell. . #collapse us_st_world = gpd.read_file(&#39;shapes/Global_LSIB_Polygons_Detailed/Global_LSIB_Polygons_Detailed.dbf&#39;) us_st_world.drop([&#39;OBJECTID&#39;, &#39;Shape_Leng&#39;, &#39;Shape_Le_1&#39;, &#39;Shape_Area&#39;], axis=1, inplace=True) us_st_world[&quot;geometry&quot;] = us_st_world.geometry.simplify(tolerance=0.05) us_st_world.to_file(&quot;world.geojson&quot;, driver=&#39;GeoJSON&#39;) . . Reading the world shapefile - . world_geojson = &#39;https://raw.githubusercontent.com/armsp/covidviz/master/assets/world.geojson&#39; us_st_world = gpd.read_file(world_geojson) . World times series covid data form JHU CSSE - . uri = &#39;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv&#39; time_s_raw = pd.read_csv(uri) time_s = time_s_raw.groupby(&#39;Country/Region&#39;).agg(dict(zip(time_s_raw.columns[4:], [&#39;sum&#39;]*(len(time_s_raw.columns)-4)))) time_s = time_s.reset_index() . Let&#39;s first find out what countries in our dataset are not present in the shapefile - . time_s[time_s[&#39;Country/Region&#39;].isin(us_st_world[&#39;COUNTRY_NA&#39;]) == False] . Country/Region 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 1/28/20 1/29/20 1/30/20 ... 6/10/20 6/11/20 6/12/20 6/13/20 6/14/20 6/15/20 6/16/20 6/17/20 6/18/20 6/19/20 . 5 Antigua and Barbuda | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 26 | 26 | 26 | 26 | 26 | 26 | 26 | 26 | 26 | 26 | . 11 Bahamas | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 103 | 103 | 103 | 103 | 103 | 103 | 104 | 104 | 104 | 104 | . 21 Bosnia and Herzegovina | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 2775 | 2832 | 2893 | 2893 | 2893 | 3040 | 3085 | 3141 | 3174 | 3273 | . 33 Central African Republic | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1888 | 1952 | 2044 | 2057 | 2057 | 2222 | 2410 | 2564 | 2605 | 2605 | . 39 Congo (Brazzaville) | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 728 | 728 | 728 | 728 | 728 | 883 | 883 | 883 | 883 | 883 | . 40 Congo (Kinshasa) | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 4390 | 4515 | 4637 | 4724 | 4778 | 4837 | 4974 | 5100 | 5283 | 5477 | . 48 Diamond Princess | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 712 | 712 | 712 | 712 | 712 | 712 | 712 | 712 | 712 | 712 | . 58 Eswatini | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 398 | 449 | 472 | 486 | 490 | 506 | 520 | 563 | 586 | 623 | . 64 Gambia | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 28 | 28 | 28 | 28 | 28 | 30 | 34 | 34 | 36 | 36 | . 75 Holy See | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 12 | 12 | 12 | 12 | 12 | 12 | 12 | 12 | 12 | 12 | . 104 MS Zaandam | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 9 | 9 | 9 | 9 | 9 | 9 | 9 | 9 | 9 | 9 | . 127 North Macedonia | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 3364 | 3538 | 3701 | 3895 | 4057 | 4157 | 4299 | 4482 | 4664 | 4820 | . 142 Saint Kitts and Nevis | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 15 | 15 | 15 | 15 | 15 | 15 | 15 | 15 | 15 | 15 | . 143 Saint Lucia | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 19 | 19 | 19 | 19 | 19 | 19 | 19 | 19 | 19 | 19 | . 144 Saint Vincent and the Grenadines | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 27 | 27 | 27 | 27 | 27 | 27 | 29 | 29 | 29 | 29 | . 146 Sao Tome and Principe | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 611 | 632 | 639 | 659 | 661 | 662 | 671 | 683 | 688 | 693 | . 165 Taiwan* | 1 | 1 | 3 | 3 | 4 | 5 | 8 | 8 | 9 | ... | 443 | 443 | 443 | 443 | 443 | 445 | 445 | 445 | 446 | 446 | . 171 Trinidad and Tobago | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 117 | 117 | 117 | 117 | 123 | 123 | 123 | 123 | 123 | 123 | . 174 US | 1 | 1 | 2 | 2 | 5 | 5 | 5 | 5 | 5 | ... | 2000702 | 2023590 | 2048986 | 2074526 | 2094058 | 2114026 | 2137731 | 2163290 | 2191052 | 2220961 | . 183 West Bank and Gaza | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 485 | 487 | 489 | 489 | 492 | 505 | 514 | 555 | 600 | 675 | . 184 Western Sahara | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 9 | 9 | 9 | 9 | 9 | 9 | 9 | 9 | 9 | 9 | . 21 rows × 151 columns . Now we need to understand that the monikers of the countries can change and that we need to figure out how to unify them and then merge them. For that let&#39;s study each of the missing countries one by one like so - . #hide_output us_st_world[us_st_world[&#39;COUNTRY_NA&#39;].str.startswith(&#39;Antigua&#39;)] . Do the same technique for all the contries and you&#39;d end up with the following modifications - . time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;Taiwan*&#39;, &#39;Country/Region&#39;] = &#39;Taiwan&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;US&#39;, &#39;Country/Region&#39;] = &#39;United States&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;Czech Republic&#39;, &#39;Country/Region&#39;] = &#39;Czechia&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;West Bank and Gaza&#39;, &#39;Country/Region&#39;] = &#39;West Bank (disp)&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;Western Sahara&#39;, &#39;Country/Region&#39;] = &#39;Western Sahara (disp)&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;Trinidad and Tobago&#39;, &#39;Country/Region&#39;] = &#39;Trinidad &amp; Tobago&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;Sao Tome and Principe&#39;, &#39;Country/Region&#39;] = &#39;Sao Tome &amp; Principe&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;Saint Vincent and the Grenadines&#39;, &#39;Country/Region&#39;] = &#39;St Vincent &amp; the Grenadines&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;Saint Lucia&#39;, &#39;Country/Region&#39;] = &#39;St Lucia&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;Saint Kitts and Nevis&#39;, &#39;Country/Region&#39;] = &#39;St Kitts &amp; Nevis&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;North Macedonia&#39;, &#39;Country/Region&#39;] = &#39;Macedonia&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;Bahamas&#39;, &#39;Country/Region&#39;] = &#39;Bahamas, The&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;Bosnia and Herzegovina&#39;, &#39;Country/Region&#39;] = &#39;Bosnia &amp; Herzegovina&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;Central African Republic&#39;, &#39;Country/Region&#39;] = &#39;Central African Rep&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;Eswatini&#39;, &#39;Country/Region&#39;] = &#39;Swaziland&#39; #time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;South Korea&#39;, &#39;Country/Region&#39;] = &#39;Korea, South&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;Congo (Kinshasa)&#39;, &#39;Country/Region&#39;] = &#39;Congo, Dem Rep of the&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;Congo (Brazzaville)&#39;, &#39;Country/Region&#39;] = &#39;Congo, Rep of the&#39; time_s.loc[time_s[&#39;Country/Region&#39;]==&#39;Antigua and Barbuda&#39;, &#39;Country/Region&#39;] = &#39;Antigua &amp; Barbuda&#39; . We will ignore the following places due to very few cases - . # collapse time_s[time_s[&#39;Country/Region&#39;].isin(us_st_world[&#39;COUNTRY_NA&#39;]) == False] . . Country/Region 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 1/28/20 1/29/20 1/30/20 ... 6/10/20 6/11/20 6/12/20 6/13/20 6/14/20 6/15/20 6/16/20 6/17/20 6/18/20 6/19/20 . 48 Diamond Princess | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 712 | 712 | 712 | 712 | 712 | 712 | 712 | 712 | 712 | 712 | . 64 Gambia | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 28 | 28 | 28 | 28 | 28 | 30 | 34 | 34 | 36 | 36 | . 75 Holy See | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 12 | 12 | 12 | 12 | 12 | 12 | 12 | 12 | 12 | 12 | . 104 MS Zaandam | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 9 | 9 | 9 | 9 | 9 | 9 | 9 | 9 | 9 | 9 | . 4 rows × 151 columns . Finding cases per day - . time_s_T = time_s.set_index(&#39;Country/Region&#39;).T time_s_T = time_s_T.apply(lambda x: x.diff(), axis=0) . Averageing the cases over a week - . # hide_output roll_case_avg_list = [] def roll_case_avg(row): #print(row) avgs = row[::-1].rolling(window=7).mean().apply(np.floor).shift(-6) roll_case_avg_list.append((row.name, avgs.iloc[0], avgs.iloc[14])) #print(avgs.iloc[1], avgs.iloc[8]) p = time_s_T.T p.apply(roll_case_avg, axis=1) . I asked the NYT GitHub Team on how they are establishing the category colors and based on their input we will use the following classification - . The thresholds for that change are: . Blue: &lt; -15% | Yellow: &gt; -15% and &lt; +15% | Light orange: &gt;+15% and &lt;+100% | Mid orange: &gt;+100% and &lt;+200% | Dark red: &gt;+200% | . Let&#39;s define a function to do that for us - . def categorize(x): if x[&#39;now&#39;] == 0 or x[&#39;ago&#39;] == 0: return &#39;Few or no cases&#39; delta = x[&#39;diff&#39;]/x[&#39;ago&#39;]*100 if delta &lt; -15: return &#39;Declining&#39; elif delta &gt; -15 and delta &lt; 15: return &#39;About the same&#39; elif delta &gt; 15 and delta &lt; 100: return &#39;Growth upto 2x&#39; elif delta &gt; 100 and delta &lt; 200: return &#39;Growth upto 3x&#39; elif delta &gt; 200: return &#39;Growth more than 3x&#39; . world_now_ago = pd.DataFrame(roll_case_avg_list, columns=[&#39;country&#39;,&#39;now&#39;,&#39;ago&#39;]) world_now_ago[&#39;diff&#39;] = world_now_ago[&#39;now&#39;] - test2[&#39;ago&#39;] world_now_ago[&#39;category&#39;] = world_now_ago.apply(categorize, axis=1) world_now_ago.groupby(&#39;category&#39;).count() . country now ago diff . category . About the same 27 | 27 | 27 | 27 | . Declining 35 | 35 | 35 | 35 | . Few or no cases 48 | 48 | 48 | 48 | . Growth more than 3x 11 | 11 | 11 | 11 | . Growth upto 2x 51 | 51 | 51 | 51 | . Growth upto 3x 14 | 14 | 14 | 14 | . world_now_ago.columns = [&#39;COUNTRY_NA&#39;, &#39;now&#39;, &#39;ago&#39;, &#39;diff&#39;, &#39;category&#39;] plot_data = us_st_world.merge(world_now_ago, how=&#39;left&#39;, on=&#39;COUNTRY_NA&#39;) . plot_data . COUNTRY_NA geometry now ago diff category . 0 Abyei (disp) | POLYGON ((29.00000 9.67356, 28.78724 9.49406, ... | NaN | NaN | NaN | NaN | . 1 Afghanistan | POLYGON ((70.98955 38.49070, 71.37353 38.25597... | 618.0 | 758.0 | -140.0 | Declining | . 2 Akrotiri (UK) | POLYGON ((32.83539 34.70576, 32.98961 34.67999... | NaN | NaN | NaN | NaN | . 3 Aksai Chin (disp) | MULTIPOLYGON (((78.69853 34.09310, 78.69837 34... | NaN | NaN | NaN | NaN | . 4 Albania | POLYGON ((19.72764 42.66045, 19.79268 42.48135... | 60.0 | 16.0 | 44.0 | Growth more than 3x | . ... ... | ... | ... | ... | ... | ... | . 274 Burma | MULTIPOLYGON (((98.03206 9.83411, 98.06033 9.8... | 3.0 | 4.0 | -1.0 | Declining | . 275 India | MULTIPOLYGON (((93.84583 7.24456, 93.96289 7.0... | 12293.0 | 8956.0 | 3337.0 | Growth upto 2x | . 276 Benin | POLYGON ((2.84088 12.40599, 3.26927 12.01606, ... | 37.0 | 5.0 | 32.0 | Growth more than 3x | . 277 Niger | POLYGON ((12.02686 23.50849, 13.52600 23.15616... | 6.0 | 1.0 | 5.0 | Growth more than 3x | . 278 Nigeria | MULTIPOLYGON (((6.13707 4.37177, 6.08799 4.359... | 566.0 | 363.0 | 203.0 | Growth upto 2x | . 279 rows × 6 columns . Now we are ready to plot the chloropleth - . # collapse base=alt.Chart(plot_data).mark_geoshape(stroke=&#39;white&#39;).transform_filter((alt.datum.COUNTRY_NA != &#39;Antarctica&#39;)).encode( color = alt.Color(&#39;category:N&#39;, scale=alt.Scale( domain=[&#39;Few or no cases&#39;, &#39;Declining&#39;, &#39;About the same&#39;, &#39;Growth upto 2x&#39;, &#39;Growth upto 3x&#39;, &#39;Growth more than 3x&#39;], range=[&#39;#f2f2f2&#39;, &#39;#badee8&#39;, &#39;#f2df91&#39;, &#39;#ffae43&#39;, &#39;#ff6e0b&#39;, &#39;#ce0a05&#39;] ), legend=alt.Legend(title=None, orient=&#39;top&#39;, labelBaseline=&#39;middle&#39;, symbolType=&#39;square&#39;, columnPadding=20, labelFontSize=15, gridAlign=&#39;each&#39;, symbolSize=200) ), tooltip = [&#39;COUNTRY_NA&#39;, alt.Tooltip(&#39;now:Q&#39;, format=&#39;.0d&#39;), alt.Tooltip(&#39;ago:Q&#39;, format=&#39;.0d&#39;), &#39;category&#39;] ).properties(height=800, width=1500).project(&#39;equalEarth&#39;).configure_view(strokeWidth=0) . . base . We can do something even more interesting...we can make the chart interactive by highlighting the countries based on their category - . Falling | Almost the same | Rising 1 | Rising 2 | Rising 3. | . #collapse selector = alt.selection_single( fields=[&#39;category&#39;], empty=&#39;all&#39;, bind=&#39;legend&#39; ) interactive = base.encode( color = alt.Color( &#39;category:N&#39;, legend=alt.Legend(values=[&#39;Declining&#39;, &#39;About the same&#39;, &#39;Growth upto 2x&#39;, &#39;Growth upto 3x&#39;, &#39;Growth more than 3x&#39;], title=None, orient=&#39;top&#39;, labelBaseline=&#39;middle&#39;, symbolType=&#39;square&#39;, columnPadding=20, labelFontSize=15, gridAlign=&#39;each&#39;, symbolSize=200), scale=alt.Scale( domain=[&#39;Few or no cases&#39;, &#39;Declining&#39;, &#39;About the same&#39;, &#39;Growth upto 2x&#39;, &#39;Growth upto 3x&#39;, &#39;Growth more than 3x&#39;], range=[&#39;#f2f2f2&#39;, &#39;#badee8&#39;, &#39;#f2df91&#39;, &#39;#ffae43&#39;, &#39;#ff6e0b&#39;, &#39;#ce0a05&#39;] ) ), opacity=alt.condition(selector, alt.value(1), alt.value(0.25)) ).add_selection( selector ) . . Now click on the legend to highlight the countries for that category. . interactive .",
            "url": "https://armsp.github.io/covidviz/geospatial,/interactive,/chloropleth,/nyt/2020/06/15/World-Case-per-day-chloropleth.html",
            "relUrl": "/geospatial,/interactive,/chloropleth,/nyt/2020/06/15/World-Case-per-day-chloropleth.html",
            "date": " • Jun 15, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Geospatial plot of cases in US",
            "content": "Today we will make our first geospatial map from the article Coronavirus in the U.S.: Latest Map and Case Count which looks like the folowing - . import geopandas as gpd import altair as alt import pandas as pd alt.renderers.set_embed_options(actions=False) # Shapefiles from us census state_shpfile = &#39;./shapes/cb_2019_us_state_20m&#39; county_shpfile = &#39;./shapes/cb_2019_us_county_20m&#39; states = gpd.read_file(state_shpfile) county = gpd.read_file(county_shpfile) # Adding longitude and latitude in state data states[&#39;lon&#39;] = states[&#39;geometry&#39;].centroid.x states[&#39;lat&#39;] = states[&#39;geometry&#39;].centroid.y # Adding longitude and latitude in county data county[&#39;lon&#39;] = county[&#39;geometry&#39;].centroid.x county[&#39;lat&#39;] = county[&#39;geometry&#39;].centroid.y . # NYT dataset county_url = &#39;https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv&#39; cdf = pd.read_csv(county_url) . cdf[cdf[&#39;fips&#39;].isnull() == True].groupby([&#39;county&#39;]).sum() . fips cases deaths . county . Joplin 0.0 | 329 | 4 | . Kansas City 0.0 | 85094 | 1700 | . New York City 0.0 | 15615980 | 1528538 | . Unknown 0.0 | 885104 | 41064 | . #hide_output cdf[cdf[&#39;fips&#39;].isnull() == True].groupby([&#39;county&#39;, &#39;state&#39;]).sum() . NYT publishes the data for New York City in a different way by combining the results of the 5 boroughs that comprise it. So we will combine them too and add a new row in the dataset with a custom fips of 1. Let&#39;s start by making this change in the raw NYT dataset itself. . cdf.loc[cdf[&#39;county&#39;] == &#39;New York City&#39;,&#39;fips&#39;] = 1 cdf[cdf[&#39;county&#39;] == &#39;New York City&#39;] . date county state fips cases deaths . 416 2020-03-01 | New York City | New York | 1.0 | 1 | 0 | . 448 2020-03-02 | New York City | New York | 1.0 | 1 | 0 | . 482 2020-03-03 | New York City | New York | 1.0 | 2 | 0 | . 518 2020-03-04 | New York City | New York | 1.0 | 2 | 0 | . 565 2020-03-05 | New York City | New York | 1.0 | 4 | 0 | . ... ... | ... | ... | ... | ... | ... | . 262876 2020-06-23 | New York City | New York | 1.0 | 217803 | 21817 | . 265930 2020-06-24 | New York City | New York | 1.0 | 218089 | 21838 | . 268988 2020-06-25 | New York City | New York | 1.0 | 218429 | 21856 | . 272054 2020-06-26 | New York City | New York | 1.0 | 218799 | 21893 | . 275123 2020-06-27 | New York City | New York | 1.0 | 219157 | 21913 | . 119 rows × 6 columns . # collapse latest_cases = cdf.groupby(&#39;fips&#39;, as_index=False).agg({&#39;county&#39;: &#39;last&#39;, &#39;date&#39;: &#39;last&#39;, &#39;state&#39;: &#39;last&#39;, &#39;cases&#39;: &#39;last&#39;, &#39;deaths&#39;: &#39;last&#39;}) latest_cases . . fips county date state cases deaths . 0 1.0 | New York City | 2020-06-27 | New York | 219157 | 21913 | . 1 1001.0 | Autauga | 2020-06-27 | Alabama | 498 | 12 | . 2 1003.0 | Baldwin | 2020-06-27 | Alabama | 555 | 10 | . 3 1005.0 | Barbour | 2020-06-27 | Alabama | 317 | 1 | . 4 1007.0 | Bibb | 2020-06-27 | Alabama | 161 | 1 | . ... ... | ... | ... | ... | ... | ... | . 3038 56037.0 | Sweetwater | 2020-06-27 | Wyoming | 81 | 0 | . 3039 56039.0 | Teton | 2020-06-27 | Wyoming | 119 | 1 | . 3040 56041.0 | Uinta | 2020-06-27 | Wyoming | 167 | 0 | . 3041 56043.0 | Washakie | 2020-06-27 | Wyoming | 38 | 5 | . 3042 56045.0 | Weston | 2020-06-27 | Wyoming | 1 | 0 | . 3043 rows × 6 columns . Now we have to make the changes in our shapefile too. For that we need to **dissolve** the 5 buroughs into one single geospatial entity. . #New York City fips = 36005&#39;, &#39;36047&#39;, &#39;36061&#39;, &#39;36081&#39;, &#39;36085 which corresponds to New York, Kings, Queens, Bronx and Richmond spatial_nyc = county[county[&#39;GEOID&#39;].isin([&#39;36005&#39;, &#39;36047&#39;, &#39;36061&#39;, &#39;36081&#39;, &#39;36085&#39;])] . combined_nyc = spatial_nyc.dissolve(by=&#39;STATEFP&#39;) alt.Chart(spatial_nyc).mark_geoshape(stroke=&#39;white&#39;, strokeWidth=3).encode() | alt.Chart(combined_nyc).mark_geoshape(stroke=&#39;white&#39;, strokeWidth=3).encode() . agg_nyc_data = spatial_nyc.dissolve(by=&#39;STATEFP&#39;).reset_index() agg_nyc_data[&#39;GEOID&#39;] = &#39;1&#39; agg_nyc_data[&#39;fips&#39;] = 1 agg_nyc_data[&#39;lon&#39;] = agg_nyc_data[&#39;geometry&#39;].centroid.x agg_nyc_data[&#39;lat&#39;] = agg_nyc_data[&#39;geometry&#39;].centroid.y . agg_nyc_data . STATEFP geometry COUNTYFP COUNTYNS AFFGEOID GEOID NAME LSAD ALAND AWATER lon lat fips . 0 36 | POLYGON ((-74.24921 40.54506, -74.21684 40.558... | 061 | 00974129 | 0500000US36061 | 1 | New York | 06 | 58690498 | 28541727 | -73.927011 | 40.695278 | 1 | . # hide_output county_nyc = gpd.GeoDataFrame(pd.concat([county, agg_nyc_data], ignore_index=True)) county_nyc[&#39;fips&#39;] = county_nyc[&#39;GEOID&#39;] county_nyc[&#39;fips&#39;] = county_nyc[&#39;fips&#39;].astype(&#39;int&#39;) county_nyc # generate FIPS in the shapefile itself by combining STATEFP and COUNTYFP #county2[&#39;STATEFP&#39;] + county2[&#39;COUNTYFP&#39;] #latest_cases[&#39;fips&#39;] = latest_cases[&#39;fips&#39;].astype(&#39;int&#39;) . latest_cases[&#39;fips&#39;].isin(county_nyc[&#39;fips&#39;]).value_counts() . True 3043 Name: fips, dtype: int64 . latest_cases[latest_cases[&#39;county&#39;] == &#39;New York City&#39;] . fips county date state cases deaths . 0 1.0 | New York City | 2020-06-27 | New York | 219157 | 21913 | . county_nyc[county_nyc[&#39;fips&#39;] == 1] . STATEFP COUNTYFP COUNTYNS AFFGEOID GEOID NAME LSAD ALAND AWATER geometry lon lat fips . 3220 36 | 061 | 00974129 | 0500000US36061 | 1 | New York | 06 | 58690498 | 28541727 | POLYGON ((-74.24921 40.54506, -74.21684 40.558... | -73.927011 | 40.695278 | 1 | . # collapse latest_cases_w_fips = county_nyc.merge(latest_cases, how=&#39;left&#39;, on=&#39;fips&#39;) circle_selection = alt.selection_single(on=&#39;mouseover&#39;, empty=&#39;none&#39;) circles = alt.Chart(latest_cases_w_fips).mark_point(fillOpacity=0.2, fill=&#39;red&#39;, strokeOpacity=1, color=&#39;red&#39;, strokeWidth=1).encode( latitude=&quot;lat:Q&quot;, longitude=&quot;lon:Q&quot;, size=alt.Size(&#39;cases:Q&#39;, scale=alt.Scale(domain=[0, 7000],),legend=alt.Legend(title=&quot;Cases&quot;)), tooltip=[&#39;county:N&#39;, &#39;cases:Q&#39;, &#39;deaths:Q&#39;], color = alt.condition(circle_selection, alt.value(&#39;black&#39;), alt.value(&#39;red&#39;)) ).project( type=&#39;albersUsa&#39; ).properties( width=1000, height=700 ).add_selection( circle_selection ) state = alt.Chart(states).mark_geoshape(fill=&#39;#ededed&#39;, stroke=&#39;white&#39;).encode( ).project( type=&#39;albersUsa&#39; ) state_text = state.mark_text().transform_filter(alt.datum.NAME != &#39;Puerto Rico&#39;).encode( longitude=&#39;lon:Q&#39;, latitude=&#39;lat:Q&#39;, text=&#39;NAME&#39;, ).project( type=&#39;albersUsa&#39; ) . . (state+circles+state_text).configure_view(strokeWidth=0) .",
            "url": "https://armsp.github.io/covidviz/geospatial/interactive/nyt/2020/06/12/US-case-counts-geospatial.html",
            "relUrl": "/geospatial/interactive/nyt/2020/06/12/US-case-counts-geospatial.html",
            "date": " • Jun 12, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Deaths above or below normal",
            "content": "We will make charts from the NYT article on What Is the Real Coronavirus Death Toll in Each State? . The charts look like the following - . Whats the purpose of this visualization? . Comparing recent totals of deaths from all causes can provide a more complete picture of the pandemic’s impact than tracking only deaths of people with confirmed diagnoses. Epidemiologists refer to fatalities in the gap between the observed and normal numbers of deaths as “excess deaths.” . Indeed, in nearly every state with an unusual number of deaths in recent weeks, that number is higher than the state’s reported number of deaths from Covid-19. On our charts, we have marked the number of official coronavirus deaths with red lines, so you can see how they match up with the total number of excess deaths. . Measuring excess deaths is crude because it does not capture all the details of how people died. But many epidemiologists believe it is the best way to measure the impact of the virus in real time. It shows how the virus is altering normal patterns of mortality where it strikes and undermines arguments that it is merely killing vulnerable people who would have died anyway. . Public health researchers use such methods to measure the impact of catastrophic events when official measures of mortality are flawed. . Measuring excess deaths does not tell us precisely how each person died. It is likely that most of the excess deaths in this period are because of the coronavirus itself, given the dangerousness of the virus and the well-documented problems with testing. But it is also possible that deaths from other causes have risen too, as hospitals have become stressed and people have been scared to seek care for ailments that are typically survivable. Some causes of death may be declining, as people stay inside more, drive less and limit their contact with others. . We will use 2 datasets to generate our chart - . The excess deaths dataset from NYT | The COVID-19 deaths dataset also from NYT | Luckily both are in the same GitHub repository - NYT Covid-19 Data However we need to do some significant preprocessing to arrive at the results. It took me a good amount of time to figure out the whole graph and once I had done it it just made so much sense :relieved: . The way these graphs are made is as follows - . First we chart the excess deaths. Excess deaths is calculated as the difference b/w all cause mortality data with expected deaths. These data are available from CDC. | NYT publishes the excess deaths data for NYC, so for starters we will use that before moving on to the other states. | Then we need to get the covid-19 related deaths from NYC which we can get from the NYT dataset or JHU CSSE dataset. | The challenge is actually combining the above. | How do we combine the above data? . The excess deaths data is weekly. With the starting and ending dates given (7 days duration), so what we are gonna do is we have to transform the COVID-19 deaths data in the same weekly format. . import pandas as pd import numpy as np import altair as alt excess_uri = &#39;https://raw.githubusercontent.com/nytimes/covid-19-data/master/excess-deaths/deaths.csv&#39; county_uri = &#39;https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv&#39; ex_df = pd.read_csv(excess_uri) co_df = pd.read_csv(county_uri) . Extracting data for NYC for year 2020 - . # collapse_show ex_nyc = ex_df[(ex_df[&#39;placename&#39;] == &#39;New York City&#39;)] ex_nyc[&#39;year&#39;] = ex_nyc[&#39;year&#39;].astype(int) ex_nyc = ex_nyc[ex_nyc[&#39;year&#39;] == 2020] . . Testing the chart - . # collapse_show excess_deaths_chart = alt.Chart(ex_nyc).mark_bar(width=5).encode( x=&#39;week:N&#39;, y=&#39;excess_deaths:Q&#39;, color = alt.condition(alt.datum.excess_deaths&gt;0, alt.value(&#39;orange&#39;), alt.value(&#39;steelblue&#39;)) ).properties(height=500, width=alt.Step(10)) . . excess_deaths_chart . This was the easy part and we can see that we are following the trends properly by comparing it with the NYT chart - . There are some differences, which I think is due to the fact that the dataset provided by NYT is actually slightly different than the data they have used to plot. Also there is actually more data used in the article than there is in the GitHub repo. So to make the charts look completely similar we will have to dig in the CDC dataset - which we will ignore for now. . Now we will transform the COVID-19 deaths data to weekly deaths data . ny_co_df = co_df[co_df[&#39;county&#39;] == &#39;New York City&#39;] ny_co_df = ny_co_df[(ny_co_df[&#39;date&#39;] &gt; &#39;2020-03-06&#39;) &amp; (ny_co_df[&#39;date&#39;] &lt; &#39;2020-05-17&#39;)] ny_co_df[&#39;death_per_day&#39;] = ny_co_df[&#39;deaths&#39;].diff() ny_co_df = ny_co_df[1:] weekly_cov_death = ny_co_df.groupby(np.arange(len(ny_co_df))//7).sum() weekly_cov_death[&#39;week&#39;] = range(11,21) . Let&#39;s plot it to see if we get the trends right - . # collapse covid_tick_deaths = alt.Chart(weekly_cov_death).mark_tick(thickness=2, color=&#39;red&#39;).encode( x=&#39;week:N&#39;, y=&#39;death_per_day:Q&#39; ) . . excess_deaths_chart + covid_tick_deaths . This looks correct. Let&#39;s beautify a little - . # collapse excess_deaths_chart = alt.Chart(ex_nyc).mark_bar(width=9).encode( x=&#39;week:N&#39;, y=&#39;excess_deaths:Q&#39;, color = alt.condition(alt.datum.excess_deaths&gt;0, alt.value(&#39;#ffab00&#39;), alt.value(&#39;#8FB8BB&#39;)) ).properties(height=500, width=alt.Step(10)) . . (excess_deaths_chart + covid_tick_deaths).configure_view(strokeWidth=0).configure_axis(grid=False) . Lets add a rectangle from March to May to complete our chart . # collapse source = alt.pd.DataFrame([{&#39;start&#39;: 11, &#39;end&#39;: 20}]) rect = alt.Chart(source).mark_rect(opacity=1, fill=&#39;#eee&#39;, xOffset=5, x2Offset=5).encode( x=&#39;start:N&#39;, x2=&#39;end:N&#39; ) . . (rect + excess_deaths_chart + covid_tick_deaths).configure_view(strokeWidth=0).configure_axis(grid=False) . We can see that it captures the trend extremely well and the datapoints are almost exactly the same. . TODO . The rest of the states using the CDC Data .",
            "url": "https://armsp.github.io/covidviz/nyt/2020/06/04/Above-Below-Normal.html",
            "relUrl": "/nyt/2020/06/04/Above-Below-Normal.html",
            "date": " • Jun 4, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Tracking the Global Outbreak: Growth Rates",
            "content": "Today we will make the growth rate charts from the NYT article on Tracking the Global Outbreak for all the countries. . . We will use the JHU CSSE dataset since NYT does not provide its own global countries dataset . #hide_output import pandas as pd import altair as alt raw_data_url = &#39;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv&#39; raw_df = pd.read_csv(raw_data_url) alt.renderers.set_embed_options(actions=False) . A few important observations - . There are some countries that have data at a finer level like state/county. For those countires we will extract the extract the corresponding rows, sum them up into a single row and transpose it to convert to a dataframe. | Those exceptional countries are &#39;Australia&#39;, &#39;Canada&#39;, &#39;China&#39;, &#39;Denmark&#39;, &#39;France&#39;, &#39;Netherlands&#39;, &#39;United Kingdom&#39; | . Let&#39;s convert the data into the desired long form from the existing wide form. . # collapse long_df = pd.DataFrame() exceptional_countries = [&#39;Australia&#39;, &#39;Canada&#39;, &#39;China&#39;, &#39;Denmark&#39;, &#39;France&#39;, &#39;Netherlands&#39;, &#39;United Kingdom&#39;] country_df_list = [] def extract_country(s): if s[1].strip() not in exceptional_countries: # print(f&quot;{s[1]} - exceptional case&quot;) # else: temp_df = pd.DataFrame(s[4:]) temp_df.columns = [&#39;value&#39;] temp_df[&#39;country&#39;] = s[1] temp_df[&#39;growth&#39;] = temp_df[&#39;value&#39;].diff() temp_df[&#39;growth&#39;][0] = temp_df[&#39;value&#39;].iloc[0] temp_df[&#39;growth&#39;] = temp_df[&#39;growth&#39;].astype(int) temp_df = temp_df.rename_axis(&#39;date&#39;).reset_index() country_df_list.append(temp_df) for country in exceptional_countries: temp_df = pd.DataFrame(raw_df[raw_df[&#39;Country/Region&#39;] == country].iloc[:,4:].sum(axis=0).rename_axis(&#39;date&#39;).reset_index()) temp_df.columns = [&#39;date&#39;,&#39;value&#39;] temp_df[&#39;country&#39;] = country temp_df[&#39;growth&#39;] = temp_df[&#39;value&#39;].diff() temp_df.loc[0, &#39;growth&#39;] = temp_df[&#39;value&#39;].iloc[0] temp_df[&#39;growth&#39;] = temp_df[&#39;growth&#39;].astype(int) country_df_list.append(temp_df) raw_df.apply(extract_country, axis=1) long_df = pd.concat(country_df_list) . . Beacause this is a large dataset, Altair will - by default - refuse to display because of possible memory issues. So we will have to enable the json transformer so that the data is passed insternally as a url. We enable it using alt.data_transformers.enable(&#39;json&#39;). Do this if you are running it locally. To do the same on Fastpages, I have already uploaded the json file that is generated by Altair behind the scenes and and I will pass the url of the file to the chart so that the output visualization is seen on the website. . # collapse #alt.data_transformers.enable(&#39;json&#39;) # use this if running locally #alt.data_transformers.disable_max_rows() # avoid this as it can hang your system url = &#39;https://raw.githubusercontent.com/armsp/covidviz/master/assets/2020-06-02-Data.json&#39; #comment this when running locally otherwise you will have old data till 1st June only a = alt.Chart().mark_bar(size=2, opacity=0.2, color=&#39;gray&#39;).transform_window( rolling_mean = &#39;mean(growth)&#39;, frame=[-6, 0], groupby=[&#39;place&#39;] ).encode( x=alt.X(&#39;date:T&#39;, title=None), y=alt.Y(&quot;growth:Q&quot;, title=None), ).properties(width=90, height=100) b = alt.Chart().mark_area(fill=&#39;red&#39;, fillOpacity=0.4).transform_window( rolling_mean = &#39;mean(growth)&#39;, frame=[-6, 0], groupby=[&#39;place&#39;] ).encode( x=alt.X(&#39;date:T&#39;), y=alt.Y(&quot;rolling_mean:Q&quot;,title=&#39;cases&#39;) ) c = b.mark_line(**{&quot;color&quot;: &quot;red&quot;, &quot;opacity&quot;: 0.7, &quot;strokeWidth&quot;: 2}) alt.layer(a, c, data=url).facet(alt.Column(&#39;country:N&#39;, title=None, sort=alt.EncodingSortField(&#39;value&#39;, op=&#39;max&#39;, order=&#39;descending&#39;), header=alt.Header(labelFontSize=13, labelColor=&#39;gray&#39;, labelFontWeight=&#39;bolder&#39;, labelAlign=&#39;center&#39;, labelAnchor=&#39;middle&#39;, labelOrient=&#39;top&#39;, labelPadding=-15, labelAngle=0)), spacing=alt.RowColnumber(row=70, column=0), title=&quot;Countrywise Distribution of Growth, Averaged over 7 days&quot;, columns=7, ).configure_axis( grid=False, #domainWidth=0.1 ).configure_view(strokeWidth=0).configure_title( fontSize=25, font=&#39;Courier&#39;, anchor=&#39;middle&#39;, color=&#39;gray&#39;, dy=-30 ) . . There are a few issues with the above chart - . We are seeing negative values in growth rate, How can it be negative? The lowest it can go is 0. | The graphs for countries with very few cases don&#39;t look good. | The scales of the countries vary a lot. We need to adjust the scale like NYT does, to make it readable. | . Let&#39;s improve upon the issues above with the following solutions - . NYT does not show graphs for those with fewer than 100 cases. Like NYT we have filtered countries with less than 100 cases | The growth rates will be negative if there are discrepancies in the data - when the cumulative cases drop for any reason than the previous day. You will certainly notice thos in the dataset for some of the countries. | We will filter the negative values as based on my observation that&#39;s what NYT seems to be doing | Independednt Y axis | . Few points to keep in mind - . The bar chart shows the increment in cases per day | The line chart is the 7 day average of growth in cases per day | The facet is ordered in descending order by the maximum number of cases | We will forcefully align the countries one below the other because choosing independent axes often leads to misaligned facet items | . # collapse a = alt.Chart().mark_bar(size=2, opacity=0.05, color=&#39;red&#39;).transform_filter( alt.datum.growth &gt;= 0).transform_filter(alt.datum.value &gt; 100).transform_window( rolling_mean = &#39;mean(growth)&#39;, frame=[-6, 0], groupby=[&#39;place&#39;] ).encode( x=alt.X(&#39;date:T&#39;, title=None), y=alt.Y(&quot;growth:Q&quot;, title=None), ).properties(width=90, height=100) b = alt.Chart().mark_area(fill=&#39;red&#39;, fillOpacity=0.04).transform_filter( alt.datum.growth &gt;= 0).transform_filter(alt.datum.value &gt; 100).transform_window( rolling_mean = &#39;mean(growth)&#39;, frame=[-6, 0], groupby=[&#39;place&#39;] ).encode( x=alt.X(&#39;date:T&#39;), y=alt.Y(&quot;rolling_mean:Q&quot;,title=&#39;cases&#39;) ) c = b.mark_line(**{&quot;color&quot;: &quot;red&quot;, &quot;opacity&quot;: 0.7, &quot;strokeWidth&quot;: 2}) alt.layer(a, b, c, data=url).facet(alt.Column(&#39;country:N&#39;, title=None, sort=alt.EncodingSortField(&#39;value&#39;, op=&#39;max&#39;, order=&#39;descending&#39;), header=alt.Header(labelFontSize=13, labelColor=&#39;gray&#39;, labelFontWeight=&#39;bolder&#39;, labelAlign=&#39;center&#39;, labelAnchor=&#39;middle&#39;, labelOrient=&#39;top&#39;, labelPadding=-15, labelAngle=0)), spacing=alt.RowColnumber(row=70, column=5), title=&quot;Countrywise Distribution of Growth, Averaged over 7 days&quot;, columns=7, align=&#39;each&#39;, ).resolve_scale(y=&#39;independent&#39;, x=&#39;independent&#39;,).configure_axis( grid=False, #domainWidth=0.1 ).configure_view(strokeWidth=0).configure_title( fontSize=25, font=&#39;Courier&#39;, anchor=&#39;middle&#39;, color=&#39;gray&#39;, dy=-30 ) . . Feel free to comment below if you didn&#39;t understand anything and I will try my best to answer .",
            "url": "https://armsp.github.io/covidviz/nyt/facet/2020/06/02/World-Map-growth.html",
            "relUrl": "/nyt/facet/2020/06/02/World-Map-growth.html",
            "date": " • Jun 2, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Missing Deaths",
            "content": "Today we will make the following graph that shows the excess deaths as appeared in the article 87,000 Missing Deaths: Tracking the True Toll of the Coronavirus Outbreak . . Fortunately the NYT provides the dataset for this in their repository. . What does Excess Death mean and how do we calculate it? . Excess deaths are estimates that include deaths from Covid-19 and other causes. Reported Covid-19 deaths reflect official coronavirus deaths during the period when all-cause mortality data is available, including figures that were later revised. . According to the github repository - . Official Covid-19 death tolls offer a limited view of the impact of the outbreak because they often exclude people who have not been tested and those who died at home. All-cause mortality is widely used by demographers and other researchers to understand the full impact of deadly events, including epidemics, wars and natural disasters. The totals in this data include deaths from Covid-19 as well as those from other causes, likely including people who could not be treated or did not seek treatment for other conditions. . . Expected Deaths . We have calculated an average number of expected deaths for each area based on historical data for the same time of year. These expected deaths are the basis for our excess death calculations, which estimate how many more people have died this year than in an average year. . The number of years used in the historical averages changes depending on what data is available, whether it is reliable and underlying demographic changes. The baselines do not adjust for changes in age or other demographics, and they do not account for changes in total population. . The number of expected deaths are not adjusted for how non-Covid-19 deaths may change during the outbreak, which will take some time to figure out. As countries impose control measures, deaths from causes like road accidents and homicides may decline. And people who die from Covid-19 cannot die later from other causes, which may reduce other causes of death. Both of these factors, if they play a role, would lead these baselines to understate, rather than overstate, the number of excess deaths. . That is what we are going to do, average the results based on the baseline field to show the blue line for expected deaths. However it also looks like they are using some sort of linear model and smoothing as mentioned in the accompanying news article - . To estimate expected deaths, we fit a linear model to reported deaths in each country from 2015 to January 2020. The model has two components — a linear time trend to account for demographic changes and a smoothing spline to account for seasonal variation. For countries limited to monthly data, the model includes month as a fixed effect rather than using a smoothing spline. . Since there isn&#39;t much information on that we will ignore it for the time being. . What&#39;s the insight that this data gives? . These numbers undermine the notion that many people who have died from the virus may soon have died anyway. In Britain, which has recorded more Covid-19 deaths than any country except the United States, 59,000 more people than usual have died since mid-March — and about 14,000 more than have been captured by official death statistics. . import pandas as pd import altair as alt url = &quot;https://raw.githubusercontent.com/nytimes/covid-19-data/master/excess-deaths/deaths.csv&quot; raw = pd.read_csv(url) . Lets study Sweden, Switzerland, UK and France for our charts . sweden = raw[raw[&#39;country&#39;] == &quot;Sweden&quot;] switzerland = raw[raw[&#39;country&#39;] == &quot;Switzerland&quot;] uk = raw[raw[&#39;country&#39;] == &quot;United Kingdom&quot;] france = raw[raw[&#39;country&#39;] == &quot;France&quot;] . Let&#39;s start with a simple layered chart - area for year 2019 and line for 2020. We will not average anything right now nor will we use all the fields in our dataset. . base = alt.Chart(sweden).encode( x=alt.X(&#39;week&#39;) ) alt.layer( base.mark_area(fill=&#39;lightblue&#39;, line=True, fillOpacity=0.3).transform_filter(&quot;datum.year == 2019&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&#39;maroon&#39;).transform_filter(&quot;datum.year == 2020&quot;).encode(y=&#39;deaths&#39;), ).properties(width=500) . For Sweden they plot the gray lines for years 2015 to 2019. The blue line is the weekly average per year and the maroon line is the deaths in 2020. . # collapse base = alt.Chart(sweden).encode( x=&#39;week&#39;, ).properties(height=200) lines = alt.layer( base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2015&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2016&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2017&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2018&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2019&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&#39;maroon&#39;).transform_filter(&quot;datum.year == 2020&quot;).encode(y=&#39;deaths&#39;), ).properties(width=400) avg = base.mark_area(fill=&#39;lightblue&#39;, line=True, fillOpacity=0.3).transform_filter(&quot;datum.year &lt; 2020&quot;).encode( y=&#39;average(deaths)&#39;, ).properties(width=500) avg + lines . . Looks like we capture the trend pretty well . Similarly for Switzerland, we will also turn off the grid and the view box - . # collapse base = alt.Chart(switzerland).encode( x=&#39;week&#39;, ).properties(height=300, width=500) lines = alt.layer( base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2015&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2016&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2017&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2018&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2019&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&#39;maroon&#39;).transform_filter(&quot;datum.year == 2020&quot;).encode(y=&#39;deaths&#39;), ).properties(width=400) avg = base.mark_area(fill=&#39;lightblue&#39;, line=True, fillOpacity=0.3).transform_filter(&quot;datum.year &lt; 2020&quot;).encode( y=&#39;average(deaths)&#39;, ) (avg+lines).configure_view(strokeWidth=0).configure_axis(grid=False) . . Trying the same for U.K - . # collapse base = alt.Chart(uk).encode( x=&#39;week&#39;, ).properties(height=300, width=550) l = alt.layer( base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2015&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2016&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2017&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2018&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(&quot;datum.year == 2019&quot;).encode(y=&#39;deaths&#39;), base.mark_line(color=&#39;maroon&#39;).transform_filter(&quot;datum.year == 2020&quot;).encode(y=&#39;deaths&#39;), ).properties(width=400) rule = base.mark_area(fill=&#39;lightblue&#39;, line=True, fillOpacity=0.3).transform_filter(&quot;datum.year &lt; 2020&quot;).encode( y=&#39;average(deaths)&#39;, ) (rule+l).configure_view(strokeWidth=0).configure_axis(grid=False) . . Let&#39;s make use of loops to do the same but for France (based on observation it looks like the gray lines are from 2015 to 2019) - . # collapse base = alt.Chart(france).encode( x=&#39;week&#39;, ).properties(height=300, width=550) avg = base.mark_area(fill=&#39;lightblue&#39;, line=True, fillOpacity=0.3).transform_filter(&quot;datum.year &lt; 2020&quot;).encode( y=&#39;average(deaths)&#39;, ) layer = [] for year in range(2015, 2021): l = base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(f&quot;datum.year == {year}&quot;).encode(y=&#39;deaths&#39;) if year == 2020: l = base.mark_line(color=&#39;maroon&#39;).transform_filter(f&quot;datum.year == {year}&quot;).encode(y=&#39;deaths&#39;) layer.append(l) alt.layer(avg,*layer).configure_view(strokeWidth=0).configure_axis(grid=False) . . The excess deaths articles and graphs update frequently and the graphics also changes quite a bit - . . In the latest versions of the charts they started using dashed lines, for that we will use strokeDash = alt.value([3,3]) . # collapse base = alt.Chart(france).encode( x=&#39;week&#39;, ).properties(height=300, width=550) avg = base.mark_area(fill=&#39;lightblue&#39;, line=True, strokeDash=[1,2], fillOpacity=0.3).transform_filter(&quot;datum.year &lt; 2020&quot;).encode( y=&#39;average(deaths)&#39;, strokeDash = alt.value([3,3]) ) layer = [] for year in range(2015, 2021): l = base.mark_line(color=&quot;gray&quot;, strokeWidth=0.5).transform_filter(f&quot;datum.year == {year}&quot;).encode(y=&#39;deaths&#39;) if year == 2020: l = base.mark_line(color=&#39;maroon&#39;).transform_filter(f&quot;datum.year == {year}&quot;).encode(y=&#39;deaths&#39;) layer.append(l) alt.layer(avg,*layer).configure_view(strokeWidth=0).configure_axis(grid=False) . .",
            "url": "https://armsp.github.io/covidviz/nyt/2020/06/01/Excess-Deaths.html",
            "relUrl": "/nyt/2020/06/01/Excess-Deaths.html",
            "date": " • Jun 1, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Covid Death Rates Graph",
            "content": "The graph that we will learn to make today is from the NYT article on Comparing Coronavirus Death Rates Across the U.S . . import altair as alt import pandas as pd url = &quot;https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-states.csv&quot; usdf = pd.read_csv(url) nydf = usdf[(usdf[&#39;state&#39;] == &quot;New York&quot;) &amp; (usdf[&#39;date&#39;] &lt; &#39;2020-04-23&#39;)] nydf[&#39;deaths_perday&#39;] = nydf[&#39;deaths&#39;].diff() chart = alt.Chart(nydf).mark_area(fill=&#39;red&#39;, fillOpacity=0.5, line=True, interpolate=&#39;step-after&#39;,).encode( x=&#39;date:T&#39;, y=&#39;deaths_perday:Q&#39;, color=alt.value(&#39;red&#39;) # Exceptional Case, bug in Vega or Vega Lite - https://stackoverflow.com/questions/62005052/how-do-i-change-the-line-color-in-altairs-filled-step-chart ).properties(height=250, width=700).configure_view( strokeWidth=0 ).configure_axis( grid=False, title=None ) chart . To highlight the region after the Stay at Home order date, we will use a Rectangle Chart - mark_react() We will also make a new source data that contains the starting date of the stay at home order and the perhaps the end date of stay at home or just the latest data in the dataset. NYT has not updated this graph and has data only till 21st or 22nd April. . # collapse chart = alt.Chart(nydf).mark_area(fill=&#39;red&#39;, fillOpacity=0.5, line=True, interpolate=&#39;step-after&#39;,).encode( x=&#39;date:T&#39;, y=&#39;deaths_perday:Q&#39;, color=alt.value(&#39;red&#39;) # Exceptional Case, bug in Vega or Vega Lite - https://stackoverflow.com/questions/62005052/how-do-i-change-the-line-color-in-altairs-filled-step-chart ).properties(height=250, width=700) . . This is the important piece of code on how to set up the Rectangle chart . source2 = [{ &quot;start&quot;: &quot;2020-03-23&quot;, &quot;end&quot;: nydf[&#39;date&#39;].max(), &quot;event&quot;: &quot;Stay at Home&quot; }] source2 = alt.pd.DataFrame(source2) rect = alt.Chart(source2).mark_rect().encode( x = &#39;start:T&#39;, x2 = &#39;end:T&#39;, color=alt.value(&#39;lightgray&#39;) ) (rect+chart).configure_view( strokeWidth=0 ).configure_axis( grid=False, title=None ) . Similarly you can do the same for any other state. Lets try Michigan for a change - . michdf = usdf[(usdf[&#39;state&#39;] == &quot;Michigan&quot;) &amp; (usdf[&#39;date&#39;] &lt; &#39;2020-04-23&#39;)] michdf[&#39;deaths_perday&#39;] = michdf[&#39;deaths&#39;].diff() . # collapse chart = alt.Chart(michdf).mark_area(fill=&#39;red&#39;, fillOpacity=0.5, line=True, interpolate=&#39;step-after&#39;,).encode( x=&#39;date:T&#39;, y=&#39;deaths_perday:Q&#39;, color=alt.value(&#39;red&#39;) # Exceptional Case, bug in Vega or Vega Lite - https://stackoverflow.com/questions/62005052/how-do-i-change-the-line-color-in-altairs-filled-step-chart ).properties(height=150, width=700) source2 = [{ &quot;start&quot;: &quot;2020-03-25&quot;, &quot;end&quot;: michdf[&#39;date&#39;].max(), &quot;event&quot;: &quot;Stay at Home&quot; }] source2 = alt.pd.DataFrame(source2) rect = alt.Chart(source2).mark_rect().encode( x = &#39;start:T&#39;, x2 = &#39;end:T&#39;, color=alt.value(&#39;lightgray&#39;) ) (rect+chart).configure_view( strokeWidth=0 ).configure_axis( grid=False, title=None ) . . The visualization till the latest date for NY would look like the following - . # collapse nydf = usdf[(usdf[&#39;state&#39;] == &quot;New York&quot;)] nydf[&#39;deaths_perday&#39;] = nydf[&#39;deaths&#39;].diff() chart = alt.Chart(nydf).mark_area(fill=&#39;red&#39;, fillOpacity=0.5, line=True, interpolate=&#39;step-after&#39;,).encode( x=&#39;date:T&#39;, y=&#39;deaths_perday:Q&#39;, color=alt.value(&#39;red&#39;) # Exceptional Case, bug in Vega or Vega Lite - https://stackoverflow.com/questions/62005052/how-do-i-change-the-line-color-in-altairs-filled-step-chart ).properties(height=250, width=700) source2 = [{ &quot;start&quot;: &quot;2020-03-23&quot;, &quot;end&quot;: nydf[&#39;date&#39;].max(), &quot;event&quot;: &quot;Stay at Home&quot; }] source2 = alt.pd.DataFrame(source2) rect = alt.Chart(source2).mark_rect().encode( x = &#39;start:T&#39;, x2 = &#39;end:T&#39;, color=alt.value(&#39;lightgray&#39;) ) (rect+chart).configure_view( strokeWidth=0 ).configure_axis( grid=False, title=None ) . .",
            "url": "https://armsp.github.io/covidviz/nyt/2020/05/31/Covid-Death-Rates.html",
            "relUrl": "/nyt/2020/05/31/Covid-Death-Rates.html",
            "date": " • May 31, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Covid Cases & Deaths Graph for U.S",
            "content": "The graph that we will learn to make today is from the NYT article on Coronavirus in the U.S . . import altair as alt import pandas as pd url = &quot;https://raw.githubusercontent.com/nytimes/covid-19-data/master/us.csv&quot; usdf = pd.read_csv(url) alt.renderers.set_embed_options(actions=False) usdf[&#39;new_deaths&#39;] = usdf[&#39;deaths&#39;].diff() usdf[&#39;new_cases&#39;] = usdf[&#39;cases&#39;].diff() # Bar Chart bar = alt.Chart().mark_bar(size=5,opacity=0.2,color=&#39;gray&#39;).encode( x=alt.X(&#39;date:T&#39;), y=alt.Y(&#39;new_deaths:Q&#39;) ) # Area Chart area = alt.Chart().mark_area(fill=&#39;gray&#39;, fillOpacity=0.15).transform_window( rolling_average=&#39;mean(new_deaths)&#39;, frame=[-6,0] # NYT uses [-6,0] for their average NOT [-7,0] ).encode( x=&#39;date:T&#39;, y=&#39;rolling_average:Q&#39; ) # Line Chart line = area.mark_line(**{&quot;color&quot;: &quot;black&quot;, &quot;opacity&quot;: 0.7, &quot;strokeWidth&quot;: 3}) # Chart of deaths deaths = (bar+area+line).properties(width=800, title=&quot;Deaths&quot;) # Bar Chart bar2 = alt.Chart().mark_bar(size=5,opacity=0.2,color=&#39;red&#39;).encode( x=alt.X(&#39;date:T&#39;), y=alt.Y(&#39;new_cases:Q&#39;) ) # Area Chart area2 = alt.Chart().mark_area(fill=&#39;red&#39;, fillOpacity=0.15).transform_window( rolling_average=&#39;mean(new_cases)&#39;, frame=[-6,0] # NYT uses [-6,0] for their average NOT [-7,0] ).encode( x=&#39;date:T&#39;, y=&#39;rolling_average:Q&#39; ) # Line Chart line2 = area2.mark_line(**{&quot;color&quot;: &quot;red&quot;, &quot;opacity&quot;: 0.7, &quot;strokeWidth&quot;: 3}) cases = (bar2+area2+line2).properties(width=800, title=&quot;Cases&quot;) # Vertically concatenate the charts alt.vconcat(cases, deaths, data=usdf).configure_axis( grid=False, title=None ).configure_view( strokeWidth=0 ) . Possible other ways to do this - . Repeat Chart | Facet Chart https://altair-viz.github.io/user_guide/compound_charts.html | .",
            "url": "https://armsp.github.io/covidviz/nyt/2020/05/30/Covid-Cases-&-Deaths-in-US.html",
            "relUrl": "/nyt/2020/05/30/Covid-Cases-&-Deaths-in-US.html",
            "date": " • May 30, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Covid Cases Graph",
            "content": "The graph that we will learn to make today is from the NYT article on Coronavirus Tracking in US which looks as follows - . . We will take the data from NYT&#39;s GitHub repo itself. Lets jump straight into the code - . import altair as alt import pandas as pd alt.renderers.set_embed_options(actions=False) url = &quot;https://raw.githubusercontent.com/nytimes/covid-19-data/master/us.csv&quot; usdf = pd.read_csv(url) usdf[&#39;new_cases&#39;] = usdf[&#39;cases&#39;].diff() # Bar Chart bar = alt.Chart(usdf).mark_bar(size=7,opacity=0.2,color=&#39;red&#39;).encode( x=alt.X(&#39;date:T&#39;), y=alt.Y(&#39;new_cases:Q&#39;) ) # Area Chart area = alt.Chart(usdf).mark_area(fill=&#39;red&#39;, fillOpacity=0.15).transform_window( rolling_average=&#39;mean(new_cases)&#39;, frame=[-6,0] # NYT uses [-6,0] for their average NOT [-7,0] ).encode( x=&#39;date:T&#39;, y=&#39;rolling_average:Q&#39; ) # Line Chart line = area.mark_line(**{&quot;color&quot;: &#39;#c11111&#39;, &quot;opacity&quot;: 0.9, &quot;strokeWidth&quot;: 5}) chart = (bar+area+line).configure_axis( grid=False, title=None ).configure_view( strokeWidth=0 ).properties(width=1000) chart . Now that we have replicated the chart effectively, lets filter the data so that we drop all the data before March as the chart from NYT shows - . That can be done in 2 ways : . using Pandas | using transform_filter on date | . You guessed it, we will use the latter. The code relevant to this would then be - . transform_filter(alt.datum.date &gt; alt.expr.toDate(&#39;2020-03-01&#39;)) . #collapse import altair as alt import pandas as pd url = &quot;https://raw.githubusercontent.com/nytimes/covid-19-data/master/us.csv&quot; usdf = pd.read_csv(url) #print(udf.columns) usdf[&#39;new_cases&#39;] = usdf[&#39;cases&#39;].diff() # Bar Chart bar = alt.Chart(usdf).mark_bar(size=7,opacity=0.15,color=&#39;red&#39;).encode( x=alt.X(&#39;date:T&#39;), y=alt.Y(&#39;new_cases:Q&#39;) ).transform_filter(alt.datum.date &gt; alt.expr.toDate(&#39;2020-03-01&#39;)) # Area Chart area = alt.Chart(usdf).mark_area(fill=&#39;red&#39;, fillOpacity=0.1).transform_window( #stroke=&#39;red&#39;, strokeWidth=2 rolling_average=&#39;mean(new_cases)&#39;, frame=[-6,0] # NYT uses [-6,0] for their average NOT [-7,0] ).encode( x=&#39;date:T&#39;, y=&#39;rolling_average:Q&#39; ).transform_filter(alt.datum.date &gt; alt.expr.toDate(&#39;2020-03-01&#39;)) # Line Chart line = area.mark_line(**{&quot;color&quot;: &quot;#c11111&quot;, &quot;opacity&quot;: 0.7, &quot;strokeWidth&quot;: 5}) k = (bar+area+line).configure_axis( grid=False, title=None ).configure_view( strokeWidth=0 ).properties(width=700)#width=alt.Step(500) k . . Now today or maybe a couple of days ago, NYT added interactivity to their charts. Lets do that too. The main concept here is using Altair Selections. For that we will use selection_single() on date field and change the opacity of the bars. So the important code pieces are - . single_bar = alt.selection_single(fields=[&#39;date&#39;], on=&#39;mouseover&#39;, empty=&#39;none&#39;) opacity = alt.condition(single_bar, alt.value(0.5), alt.value(0.15)) . #collapse import altair as alt import pandas as pd url = &quot;https://raw.githubusercontent.com/nytimes/covid-19-data/master/us.csv&quot; usdf = pd.read_csv(url) #print(udf.columns) usdf[&#39;new_cases&#39;] = usdf[&#39;cases&#39;].diff() single_bar = alt.selection_single(fields=[&#39;date&#39;], on=&#39;mouseover&#39;, empty=&#39;none&#39;) # Bar Chart bar = alt.Chart(usdf).mark_bar(size=7,opacity=0.15,color=&#39;red&#39;).encode( x=alt.X(&#39;date:T&#39;), y=alt.Y(&#39;new_cases:Q&#39;), opacity= alt.condition(single_bar, alt.value(0.5), alt.value(0.15)) ).add_selection(single_bar).transform_filter(alt.datum.date &gt; alt.expr.toDate(&#39;2020-03-01&#39;)) # Area Chart area = alt.Chart(usdf).mark_area(fill=&#39;red&#39;, fillOpacity=0.1).transform_window( rolling_average=&#39;mean(new_cases)&#39;, frame=[-6,0] # NYT uses [-6,0] for their average NOT [-7,0] ).encode( x=&#39;date:T&#39;, y=&#39;rolling_average:Q&#39; ).transform_filter(alt.datum.date &gt; alt.expr.toDate(&#39;2020-03-01&#39;)) # Line Chart line = area.mark_line(**{&quot;color&quot;: &quot;#c11111&quot;, &quot;opacity&quot;: 0.7, &quot;strokeWidth&quot;: 5}) chart = (bar+area+line).configure_axis( grid=False, title=None ).configure_view( strokeWidth=0 ).properties(width=700) chart . . Isn&#39;t that amazing . TODO . [ ] Adding tooltip/text to interactive bar chart | .",
            "url": "https://armsp.github.io/covidviz/nyt/2020/05/29/Covid-cases.html",
            "relUrl": "/nyt/2020/05/29/Covid-cases.html",
            "date": " • May 29, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "What is Altair?",
            "content": "To understand Altair we need to know what is it built on. The following hierarchy enables us to understand that - . D3 | Vega | Vega-Lite | Altair | . Lets start with Vega. . What is Vega? . Vega is a visualization grammar, a declarative language for creating, saving, and sharing interactive visualization designs. With Vega, you can describe the visual appearance and interactive behavior of a visualization in a JSON format, and generate web-based views using Canvas or SVG. . . Note: visualization grammar . Where does D3 fit in here? . To be clear, Vega is not intended as a “replacement” for D3. D3 is intentionally a lower-level library. During the early design of D3, we even referred to it as a “visualization kernel” rather than a “toolkit” or “framework”. In addition to custom design, D3 is intended as a supporting layer for higher-level visualization tools. Vega is one such tool, and Vega uses D3 heavily within its implementation. . Vega provides a higher-level visualization specification language on top of D3. By design, D3 will maintain an “expressivity advantage” and in some cases will be better suited for novel design ideas. On the other hand, we intend Vega to be convenient for a wide range of common yet customizable visualizations. . Now, that we know the top hierarchy, the rest is easy to follow - . Vega-Lite . Vega-Lite is a high-level grammar of interactive graphics. It provides a concise JSON syntax for rapidly generating visualizations to support analysis. Vega-Lite specifications can be compiled to Vega specifications. . . Note: high-level visualization grammar . Vega-Lite is used by some big players - . | | | | | | | | | | | | . Altair . In short, Altair exposes a Python API for building statistical visualizations that follows Vega-Lite syntax. . Altair is a declarative statistical visualization library for Python, based on Vega and Vega-Lite. With Altair, you can spend more time understanding your data and its meaning. . Altair’s API is simple, friendly and consistent and built on top of the powerful Vega-Lite visualization grammar. This elegant simplicity produces beautiful and effective visualizations with a minimal amount of code. . . I do not intend to teach you how to use Altair or Vega Lite. You can do that on your own using the following excellent resources - . Conference Talk by Jake VanderPlas | . Exploratory Data Visualization with Altair | UW Data Viz Curriculum | .",
            "url": "https://armsp.github.io/covidviz/2020/05/28/Altair-Intro.html",
            "relUrl": "/2020/05/28/Altair-Intro.html",
            "date": " • May 28, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi :wave: . My name is Shantam. This blog will contain my attempts to replicate/emulate the visualizations at large media houses, related to COVID-19 pandemic, using open source data &amp; tools. . I am a driven python developer and data scientist. I have worked with Machine Learning and Big Data too. Did I tell you I am a published poet too? . I am available for consulting, collaborations, part time or full time jobs. Let me know if you want to talk. . If you want me to try a specific visualization, do let me know by raising an issue in the GitHub Repository. . My website - www.shantamraj.com . I am working on a project that you should definitely check out - COVID-19 Stories .",
          "url": "https://armsp.github.io/covidviz/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://armsp.github.io/covidviz/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}